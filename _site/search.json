[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Recognizing the legacy of the course and Essex, this was the 2022 About…\nHarold D. Clarke was Ashbel Smith Professor in the School of Economic, Political and Policy Sciences, University of Texas at Dallas, and adjunct Professor, Department of Government, University of Essex. His current research interests focus on the political economy of party support. He has published widely on this topic in journals such as the American Journal of Political Science, American Political Science Review, and British Journal of Political Science. He is chief editor of Electoral Studies. He has been a principal investigator for the 2001, 2005 and 2010 British Election Study (University of Essex and University of Texas at Dallas), the 2011 Political Support in Canada Study, and the 2012 Political Support in America Study. His most recent books are Brexit—Why Britain Voted to Leave the European Union (Cambridge University Press, 2017), Affluence, Austerity and Electoral Change in Britain (Cambridge University Press, 2013), and Austerity and Political Choice in Britain (Palgrave Macmillan, 2015).\nRobert W. Walker is Associate Professor of Quantitative Methods in the Atkinson Graduate School of Management at Willamette University where he teaches statistics and data science. He earned a Ph. D. in political science from the University of Rochester in 2005 and has previously held teaching positions at Dartmouth College, Rice University, Texas A&M University, and Washington University in Saint Louis. He researches distributed lag variants of the between-within model and semi-Markov processes for time-series, cross-section data in international relations and international/comparative political economy. He previously taught four iterations in the U. S. National Science Foundation funded Empirical Implications of Theoretical Models sequence at Washington University in Saint Louis and has been an honorary instructor in panel data at the Essex Summer School since 2010. His work with Curt Signorino and Muhammet Bas was awarded the Miller Prize for the best article in Political Analysis in 2009."
  },
  {
    "objectID": "posts/day-9/index.html",
    "href": "posts/day-9/index.html",
    "title": "Day 9: DPD and GMM",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 9: DPD and GMM"
    ]
  },
  {
    "objectID": "posts/day-9/index.html#slides",
    "href": "posts/day-9/index.html#slides",
    "title": "Day 9: DPD and GMM",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 9: DPD and GMM"
    ]
  },
  {
    "objectID": "posts/day-9/index.html#r-and-gmm",
    "href": "posts/day-9/index.html#r-and-gmm",
    "title": "Day 9: DPD and GMM",
    "section": "R and GMM",
    "text": "R and GMM\nThere are a number of packages for estimating GMM models in R. pgmm does many of the most basic models, though Stata is the industry leader in this. That said, there is a relatively new and highly customizable package for this: pdynmc which is described here in a pdf vignette. The package is rather new; the paper is forthcoming.",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 9: DPD and GMM"
    ]
  },
  {
    "objectID": "posts/day-7/index.html",
    "href": "posts/day-7/index.html",
    "title": "Day 7: Missing Data",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#slides",
    "href": "posts/day-7/index.html#slides",
    "title": "Day 7: Missing Data",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#missing-data",
    "href": "posts/day-7/index.html#missing-data",
    "title": "Day 7: Missing Data",
    "section": "Missing Data",
    "text": "Missing Data\nuse \"https://github.com/robertwwalker/Essex-Data/raw/main/ISQ99-Essex.dta\"\nxtset\ndrop if AINEW==.\nxtset\n* Give consideration to what we should do about the Lagged DV\ndrop if AILAG==.\n* A note about long versus flong\nmi set flong\n* Note that it adds some variables; what are they?\nmi describe\nmi misstable summarize\n* There are some logical restrictions that we are going to want.\n* For example, we have some changes that need to be consistent\n* with levels.  We can deal with that; just calculate the changes\n* after imputation.\nmi register imputed DEMOC3\nmi impute regress DEMOC3 PERCHPCG PERCHPOP LPOP CWARCOW IWARCOW2, add(10) dots force\n* mi impute regress PCGTHOU DEMOC3 PERCHPOP LPOP CWARCOW IWARCOW2, add(20)\nmi estimate: xtreg AINEW AILAG DEMOC3, fe",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#the-data-poe-tate-keith-1999",
    "href": "posts/day-7/index.html#the-data-poe-tate-keith-1999",
    "title": "Day 7: Missing Data",
    "section": "The Data: Poe, Tate, Keith 1999",
    "text": "The Data: Poe, Tate, Keith 1999\nTwo objectives. Load the data and transform it into a pdata.frame for now.\n\nknitr::opts_chunk$set(warning=FALSE, message=FALSE, comment=NA, prompt=FALSE)\nlibrary(broom)\nlibrary(plm)\nlibrary(foreign)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::between() masks plm::between()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks plm::lag(), stats::lag()\n✖ dplyr::lead()    masks plm::lead()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nISQ99_Essex &lt;- read.dta(\"./img/ISQ99-Essex.dta\")\nISQ99p &lt;- pdata.frame(ISQ99_Essex, c(\"IDORIGIN\", \"YEAR\"))",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#pdata.frame",
    "href": "posts/day-7/index.html#pdata.frame",
    "title": "Day 7: Missing Data",
    "section": "pdata.frame",
    "text": "pdata.frame\nWill allow R to answer many questions that stata’s xt commands make available. First, some basic summaries to get to balance.\n\nsummary(ISQ99_Essex)\n\n    IDORIGIN          YEAR            AI              SD            POLRT     \n Min.   :  2.0   Min.   :1976   Min.   :1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:290.0   1st Qu.:1980   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.00  \n Median :435.0   Median :1984   Median :3.000   Median :2.000   Median :3.00  \n Mean   :446.7   Mean   :1984   Mean   :2.753   Mean   :2.241   Mean   :3.81  \n 3rd Qu.:640.0   3rd Qu.:1989   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:6.00  \n Max.   :990.0   Max.   :1993   Max.   :5.000   Max.   :5.000   Max.   :7.00  \n                                NA's   :1061    NA's   :587     NA's   :382   \n      MIL2             LEFT             BRIT            PCGNP      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :   52  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  390  \n Median :0.0000   Median :0.0000   Median :0.0000   Median : 1112  \n Mean   :0.2725   Mean   :0.1764   Mean   :0.3554   Mean   : 3592  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.: 3510  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :36670  \n NA's   :382      NA's   :393      NA's   :290      NA's   :443    \n     AINEW           SDNEW           IDGURR          AILAG          SDLAG      \n Min.   :1.000   Min.   :1.000   Min.   :  2.0   Min.   :1.00   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:290.0   1st Qu.:1.00   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :450.0   Median :2.00   Median :2.000  \n Mean   :2.443   Mean   :2.262   Mean   :455.8   Mean   :2.45   Mean   :2.247  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:663.0   3rd Qu.:3.00   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :990.0   Max.   :5.00   Max.   :5.000  \n NA's   :468     NA's   :468                     NA's   :644    NA's   :644    \n    PERCHPCG          PERCHPOP            LPOP          PCGTHOU      \n Min.   :-95.500   Min.   :-48.450   Min.   :11.00   Min.   : 0.050  \n 1st Qu.: -2.545   1st Qu.:  0.910   1st Qu.:14.51   1st Qu.: 0.390  \n Median :  4.615   Median :  2.220   Median :15.59   Median : 1.110  \n Mean   :  4.614   Mean   :  2.193   Mean   :15.48   Mean   : 3.592  \n 3rd Qu.: 11.760   3rd Qu.:  2.940   3rd Qu.:16.64   3rd Qu.: 3.510  \n Max.   :128.570   Max.   :126.010   Max.   :20.89   Max.   :36.670  \n NA's   :618       NA's   :293       NA's   :115     NA's   :443     \n     DEMOC3          CWARCOW         IWARCOW2     \n Min.   : 0.000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.: 0.000   1st Qu.:0.000   1st Qu.:0.0000  \n Median : 0.000   Median :0.000   Median :0.0000  \n Mean   : 3.682   Mean   :0.092   Mean   :0.0862  \n 3rd Qu.: 9.000   3rd Qu.:0.000   3rd Qu.:0.0000  \n Max.   :10.000   Max.   :1.000   Max.   :1.0000  \n NA's   :793      NA's   :407     NA's   :380     \n\ntable(ISQ99_Essex$IDORIGIN)\n\n\n    2    20    31    40    41    42    51    52    53    55    56    57    59 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n   70    90    91    92    93    94    95    96   100   101   110   115   130 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  135   140   145   150   155   160   165   200   205   210   211   212   220 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  225   230   235   260   265   290   305   310   315   316   325   333   339 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  345   346   347   348   349   350   352   355   360   365   366   367   368 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  369   370   371   372   373   374 374.1 374.2 374.3 374.4 374.5   375   380 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  385   390   395   402   403   404   411   420   432   433   434   435   436 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  437   438   439   450   451   452   461   471   475   481   482   483   484 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  490   500   501   510   516   517   520   525   530   531   540   541   551 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  552   553   560   562   570   571   572   580   581   590   591   600   615 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  616   620   625   630   640   645   651   652   660   663   666   670   678 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  680   690   692   694   696   698   700   710   712   713   732   740   750 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  760   770   771   775   780   781   790   800   811   812   815   820   821 \n   18    18    18    18    18    18    18    18    18    18    18    18    18 \n  830   840   850   900   910   911   914   920   950   990 \n   18    18    18    18    18    18    18    18    18    18 \n\ntable(ISQ99_Essex$YEAR)\n\n\n1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 \n 179  179  179  179  179  179  179  179  179  179  179  179  179  179  179  179 \n1992 1993 \n 179  179 \n\nis.pbalanced(ISQ99p)\n\n[1] TRUE\n\n\nIs any variable time invariant?\n\npvar(ISQ99p)\n\nno time variation:       IDORIGIN BRIT IDGURR AILAG SDLAG PERCHPCG PERCHPOP \nno individual variation: YEAR AI SD POLRT MIL2 LEFT BRIT PCGNP AINEW SDNEW AILAG SDLAG PERCHPCG PERCHPOP LPOP PCGTHOU DEMOC3 CWARCOW IWARCOW2 \nall NA in time dimension for at least one individual:  AI SD POLRT MIL2 LEFT BRIT PCGNP AINEW SDNEW AILAG SDLAG PERCHPCG PERCHPOP LPOP PCGTHOU DEMOC3 CWARCOW IWARCOW2 \nall NA in ind. dimension for at least one time period: AI SD POLRT MIL2 LEFT BRIT PCGNP AINEW SDNEW AILAG SDLAG PERCHPCG PERCHPOP LPOP PCGTHOU DEMOC3 CWARCOW IWARCOW2",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#visualizations-of-missingness",
    "href": "posts/day-7/index.html#visualizations-of-missingness",
    "title": "Day 7: Missing Data",
    "section": "Visualizations of Missingness",
    "text": "Visualizations of Missingness\n\nlibrary(visdat)\nvis_dat(ISQ99_Essex)\n\n\n\n\n\n\n\n\n\nvis_miss(ISQ99_Essex)",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#univariate",
    "href": "posts/day-7/index.html#univariate",
    "title": "Day 7: Missing Data",
    "section": "Univariate",
    "text": "Univariate\n\nlibrary(ggplot2)\nlibrary(naniar)\nmplot &lt;- gg_miss_var(ISQ99_Essex)\nmplot",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#bivariate",
    "href": "posts/day-7/index.html#bivariate",
    "title": "Day 7: Missing Data",
    "section": "Bivariate",
    "text": "Bivariate\n\nmplot &lt;- ggplot(ISQ99_Essex, \n       aes(x = DEMOC3, \n           y = POLRT)) + \n  geom_point()\nmplot\n\n\n\n\n\n\n\n\n\nlibrary(naniar)\nmplot &lt;- ggplot(ISQ99_Essex, \n       aes(x = DEMOC3, \n           y = POLRT)) + \n  geom_miss_point()\nmplot",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#multiple-imputation-amelia-ii",
    "href": "posts/day-7/index.html#multiple-imputation-amelia-ii",
    "title": "Day 7: Missing Data",
    "section": "Multiple Imputation: Amelia II",
    "text": "Multiple Imputation: Amelia II\nA simple multivariate normal is easy as long as the data are well behaved. NB: This uses none of the time series or cross-sectional dimensionality for identification.\n\nlibrary(Amelia)\n#library(Zelig)\n# Shorten the dataset\nISQ99.4MI &lt;- ISQ99_Essex[,c(1,2,5:11,13:21)]\na.out &lt;- amelia(ISQ99.4MI, m=5, ts=\"YEAR\", cs=\"IDORIGIN\", noms=c(\"MIL2\",\"LEFT\",\"BRIT\",\"CWARCOW\",\"IWARCOW2\"), ords = c(\"AINEW\",\"SDNEW\",\"AILAG\",\"SDLAG\",\"POLRT\"))\n\nWarning: There are observations in the data that are completely missing. \n         These observations will remain unimputed in the final datasets. \n-- Imputation 1 --\n\n  1  2  3  4  5  6  7\n\n-- Imputation 2 --\n\n  1  2  3  4  5  6  7  8  9\n\n-- Imputation 3 --\n\n  1  2  3  4  5  6  7  8\n\n-- Imputation 4 --\n\n  1  2  3  4  5  6  7  8  9 10\n\n-- Imputation 5 --\n\n  1  2  3  4  5  6\n\n\nWithout simplifying, it crashes because there are id variables of different sorts and other things hiding in there, perfect multicollinearities exist. Even with them, we need a bit more work.",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#transforms-are-key",
    "href": "posts/day-7/index.html#transforms-are-key",
    "title": "Day 7: Missing Data",
    "section": "Transforms are Key",
    "text": "Transforms are Key",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#using-the-cs-and-ts-information-with-polytimesplinetime-and-intercs",
    "href": "posts/day-7/index.html#using-the-cs-and-ts-information-with-polytimesplinetime-and-intercs",
    "title": "Day 7: Missing Data",
    "section": "Using the cs and ts information with polytime/splinetime and intercs",
    "text": "Using the cs and ts information with polytime/splinetime and intercs\n\na.out2 &lt;- amelia(ISQ99.4MI, m=5, ts=\"YEAR\", cs=\"IDORIGIN\", polytime=2, intercs=FALSE, noms=c(\"MIL2\",\"LEFT\",\"BRIT\",\"CWARCOW\",\"IWARCOW2\"), ords = c(\"AINEW\",\"SDNEW\",\"AILAG\",\"SDLAG\",\"POLRT\"), p2s=2, emburn = c(20,50))\n\n\namelia starting\nbeginning prep functions\nVariables used:  POLRT PCGNP AINEW SDNEW AILAG SDLAG PERCHPCG PERCHPOP LPOP PCGTHOU DEMOC3 noms.MIL2.2 noms.LEFT.2 noms.BRIT.2 noms.CWARCOW.2 noms.IWARCOW2.2 time.1 time.2 \nrunning bootstrap\n-- Imputation 1 --\nsetting up EM chain indicies\n\n  1(189)  2(182)  3(155)  4(89)  5(13)  6(9)  7(6)  8(1)  9(0) 10(0) 11(0) 12(0) 13(0) 14(0) 15(0) 16(0) 17(0) 18(0) 19(0) 20\n(0)\n\n saving and cleaning\n\nrunning bootstrap\n-- Imputation 2 --\nsetting up EM chain indicies\n\n  1(189)  2(182)  3(162)  4(87)  5(14)  6(5)  7(1)  8(0)  9(0) 10(0) 11(0) 12(0) 13(0) 14(0) 15(0) 16(0) 17(0) 18(0) 19(0) 20\n(0)\n\n saving and cleaning\n\nrunning bootstrap\n-- Imputation 3 --\nsetting up EM chain indicies\n\n  1(188)  2(183)  3(157)  4(76)  5(29)  6(17)  7(8)  8(0)  9(0) 10(0) 11(0) 12(0) 13(0) 14(0) 15(0) 16(0) 17(0) 18(0) 19(0) 20\n(0)\n\n saving and cleaning\n\nrunning bootstrap\n-- Imputation 4 --\nsetting up EM chain indicies\n\n  1(189)  2(181)  3(153)  4(60)  5(20)  6(13)  7(7)  8(2)  9(0) 10(0) 11(0) 12(0) 13(0) 14(0) 15(0) 16(0) 17(0) 18(0) 19(0) 20\n(0)\n\n saving and cleaning\n\nrunning bootstrap\n-- Imputation 5 --\nsetting up EM chain indicies\n\n  1(189)  2(181)  3(160)  4(90)  5(25)  6(12)  7(3)  8(0)  9(0) 10(0) 11(0) 12(0) 13(0) 14(0) 15(0) 16(0) 17(0) 18(0) 19(0) 20\n(0)\n\n saving and cleaning\n\nsummary(a.out2)\n\n\nAmelia output with 5 imputed datasets.\nReturn code:  1 \nMessage:  Normal EM convergence. \n\nChain Lengths:\n--------------\nImputation 1:  20\nImputation 2:  20\nImputation 3:  20\nImputation 4:  20\nImputation 5:  20\n\nRows after Listwise Deletion:  2144 \nRows after Imputation:  3222 \nPatterns of missingness in the data:  65 \n\nFraction Missing for original variables: \n-----------------------------------------\n\n         Fraction Missing\nIDORIGIN       0.00000000\nYEAR           0.00000000\nPOLRT          0.11855990\nMIL2           0.11855990\nLEFT           0.12197393\nBRIT           0.09000621\nPCGNP          0.13749224\nAINEW          0.14525140\nSDNEW          0.14525140\nAILAG          0.19987585\nSDLAG          0.19987585\nPERCHPCG       0.19180633\nPERCHPOP       0.09093731\nLPOP           0.03569212\nPCGTHOU        0.13749224\nDEMOC3         0.24612042\nCWARCOW        0.12631906\nIWARCOW2       0.11793917\n\n\nNow let’s analyze it.",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-7/index.html#more-general",
    "href": "posts/day-7/index.html#more-general",
    "title": "Day 7: Missing Data",
    "section": "More General",
    "text": "More General\n\nall_imputations &lt;- dplyr::bind_rows(unclass(a.out2$imputations), .id = \"m\") %&gt;%\n  group_by(m) %&gt;%\n  nest()\nall_imputations\n\n# A tibble: 5 × 2\n# Groups:   m [5]\n  m     data                 \n  &lt;chr&gt; &lt;list&gt;               \n1 imp1  &lt;tibble [3,222 × 18]&gt;\n2 imp2  &lt;tibble [3,222 × 18]&gt;\n3 imp3  &lt;tibble [3,222 × 18]&gt;\n4 imp4  &lt;tibble [3,222 × 18]&gt;\n5 imp5  &lt;tibble [3,222 × 18]&gt;\n\n\n\nmodels_imputations &lt;- all_imputations %&gt;%\n  mutate(model = data %&gt;% map(~ lm(AINEW ~ AILAG + MIL2 + LEFT + BRIT + CWARCOW + IWARCOW2 + PCGTHOU + PERCHPOP + DEMOC3, data = .)),\n         tidied = model %&gt;% map(~ tidy(., conf.int = TRUE)),\n         glance = model %&gt;% map(~ glance(.)))\n\nmodels_imputations\n\n# A tibble: 5 × 5\n# Groups:   m [5]\n  m     data                  model  tidied            glance           \n  &lt;chr&gt; &lt;list&gt;                &lt;list&gt; &lt;list&gt;            &lt;list&gt;           \n1 imp1  &lt;tibble [3,222 × 18]&gt; &lt;lm&gt;   &lt;tibble [10 × 7]&gt; &lt;tibble [1 × 12]&gt;\n2 imp2  &lt;tibble [3,222 × 18]&gt; &lt;lm&gt;   &lt;tibble [10 × 7]&gt; &lt;tibble [1 × 12]&gt;\n3 imp3  &lt;tibble [3,222 × 18]&gt; &lt;lm&gt;   &lt;tibble [10 × 7]&gt; &lt;tibble [1 × 12]&gt;\n4 imp4  &lt;tibble [3,222 × 18]&gt; &lt;lm&gt;   &lt;tibble [10 × 7]&gt; &lt;tibble [1 × 12]&gt;\n5 imp5  &lt;tibble [3,222 × 18]&gt; &lt;lm&gt;   &lt;tibble [10 × 7]&gt; &lt;tibble [1 × 12]&gt;\n\n\n\nparams &lt;- models_imputations %&gt;%\n  unnest(tidied) %&gt;%\n  select(m, term, estimate, std.error) %&gt;%\n  gather(key, value, estimate, std.error) %&gt;%\n  spread(term, value)\nparams\n\n# A tibble: 10 × 12\n# Groups:   m [5]\n   m     key      `(Intercept)`  AILAG    BRIT CWARCOW   DEMOC3 IWARCOW2    LEFT\n   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 imp1  estimate        1.08   0.607  -0.106   0.583  -0.0314    0.187  -0.0867\n 2 imp1  std.err…        0.0485 0.0136  0.0279  0.0454  0.00380   0.0456  0.0362\n 3 imp2  estimate        1.04   0.623  -0.125   0.520  -0.0334    0.201  -0.114 \n 4 imp2  std.err…        0.0462 0.0129  0.0267  0.0436  0.00364   0.0443  0.0353\n 5 imp3  estimate        0.999  0.637  -0.121   0.537  -0.0281    0.106  -0.0685\n 6 imp3  std.err…        0.0455 0.0128  0.0264  0.0432  0.00364   0.0431  0.0343\n 7 imp4  estimate        1.03   0.625  -0.116   0.503  -0.0313    0.162  -0.0360\n 8 imp4  std.err…        0.0469 0.0132  0.0271  0.0458  0.00373   0.0441  0.0348\n 9 imp5  estimate        1.05   0.615  -0.0938  0.521  -0.0291    0.245  -0.0686\n10 imp5  std.err…        0.0465 0.0130  0.0269  0.0445  0.00367   0.0428  0.0351\n# ℹ 3 more variables: MIL2 &lt;dbl&gt;, PCGTHOU &lt;dbl&gt;, PERCHPOP &lt;dbl&gt;\n\n\n\njust_coefs &lt;- params %&gt;%\n  filter(key == \"estimate\") %&gt;%\n  ungroup() %&gt;%\n  select(-m, -key)\njust_coefs\n\n# A tibble: 5 × 10\n  `(Intercept)` AILAG    BRIT CWARCOW  DEMOC3 IWARCOW2    LEFT   MIL2 PCGTHOU\n          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1         1.08  0.607 -0.106    0.583 -0.0314    0.187 -0.0867 0.112  -0.0163\n2         1.04  0.623 -0.125    0.520 -0.0334    0.201 -0.114  0.0874 -0.0142\n3         0.999 0.637 -0.121    0.537 -0.0281    0.106 -0.0685 0.133  -0.0162\n4         1.03  0.625 -0.116    0.503 -0.0313    0.162 -0.0360 0.113  -0.0146\n5         1.05  0.615 -0.0938   0.521 -0.0291    0.245 -0.0686 0.106  -0.0193\n# ℹ 1 more variable: PERCHPOP &lt;dbl&gt;\n\n\n\njust_ses &lt;- params %&gt;%\n  filter(key == \"std.error\") %&gt;%\n  ungroup() %&gt;%\n  select(-m, -key)\njust_ses\n\n# A tibble: 5 × 10\n  `(Intercept)`  AILAG   BRIT CWARCOW  DEMOC3 IWARCOW2   LEFT   MIL2 PCGTHOU\n          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1        0.0485 0.0136 0.0279  0.0454 0.00380   0.0456 0.0362 0.0316 0.00255\n2        0.0462 0.0129 0.0267  0.0436 0.00364   0.0443 0.0353 0.0305 0.00250\n3        0.0455 0.0128 0.0264  0.0432 0.00364   0.0431 0.0343 0.0301 0.00246\n4        0.0469 0.0132 0.0271  0.0458 0.00373   0.0441 0.0348 0.0312 0.00251\n5        0.0465 0.0130 0.0269  0.0445 0.00367   0.0428 0.0351 0.0308 0.00248\n# ℹ 1 more variable: PERCHPOP &lt;dbl&gt;\n\n\n\ncoefs_melded &lt;- mi.meld(just_coefs, just_ses)\ncoefs_melded\n\n$q.mi\n     (Intercept)     AILAG       BRIT  CWARCOW      DEMOC3  IWARCOW2\n[1,]    1.040136 0.6214036 -0.1124235 0.532683 -0.03064904 0.1803708\n            LEFT      MIL2     PCGTHOU      PERCHPOP\n[1,] -0.07470803 0.1102575 -0.01613081 -0.0009362572\n\n$se.mi\n     (Intercept)      AILAG       BRIT    CWARCOW      DEMOC3   IWARCOW2\n[1,]  0.05578001 0.01796531 0.03039278 0.05577991 0.004347858 0.07137014\n           LEFT       MIL2     PCGTHOU    PERCHPOP\n[1,] 0.04696314 0.03575381 0.003344076 0.003508067\n\ncoefs_melded$q.mi / coefs_melded$se.mi\n\n     (Intercept)    AILAG      BRIT  CWARCOW    DEMOC3 IWARCOW2     LEFT\n[1,]    18.64711 34.58908 -3.699019 9.549729 -7.049228 2.527259 -1.59078\n         MIL2   PCGTHOU   PERCHPOP\n[1,] 3.083796 -4.823697 -0.2668869",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 7: Missing Data"
    ]
  },
  {
    "objectID": "posts/day-1/index.html",
    "href": "posts/day-1/index.html",
    "title": "Day 1: Time, Decomposition, and Summaries",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 1: Time, Decomposition, and Summaries"
    ]
  },
  {
    "objectID": "posts/day-1/index.html#slides",
    "href": "posts/day-1/index.html#slides",
    "title": "Day 1: Time, Decomposition, and Summaries",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 1: Time, Decomposition, and Summaries"
    ]
  },
  {
    "objectID": "posts/day-1/index.html#basic-r-commands",
    "href": "posts/day-1/index.html#basic-r-commands",
    "title": "Day 1: Time, Decomposition, and Summaries",
    "section": "Basic R commands",
    "text": "Basic R commands\nLoading the data in R and a summary using skimr::skim.\n\nlibrary(haven); library(kableExtra)\nHR.Data &lt;- read_dta(url(\"https://github.com/robertwwalker/DADMStuff/raw/master/ISQ99-Essex.dta\"))\nlibrary(skimr)\nskim(HR.Data) %&gt;% kable() %&gt;% scroll_box(width=\"80%\", height=\"50%\")\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nnumeric\nIDORIGIN\n0\n1.0000000\n446.7178771\n243.1931782\n2.00\n290.000\n435.000\n640.00\n990.00\n▆▇▇▆▂\n\n\nnumeric\nYEAR\n0\n1.0000000\n1984.5000000\n5.1889328\n1976.00\n1980.000\n1984.500\n1989.00\n1993.00\n▇▆▇▆▇\n\n\nnumeric\nAI\n1061\n0.6707014\n2.7533549\n1.0752989\n1.00\n2.000\n3.000\n3.00\n5.00\n▃▇▇▃▂\n\n\nnumeric\nSD\n587\n0.8178150\n2.2406072\n1.1303528\n1.00\n1.000\n2.000\n3.00\n5.00\n▇▇▆▂▁\n\n\nnumeric\nPOLRT\n382\n0.8814401\n3.8095070\n2.2230297\n1.00\n2.000\n3.000\n6.00\n7.00\n▇▂▂▁▇\n\n\nnumeric\nMIL2\n382\n0.8814401\n0.2725352\n0.4453421\n0.00\n0.000\n0.000\n1.00\n1.00\n▇▁▁▁▃\n\n\nnumeric\nLEFT\n393\n0.8780261\n0.1763874\n0.3812168\n0.00\n0.000\n0.000\n0.00\n1.00\n▇▁▁▁▂\n\n\nnumeric\nBRIT\n290\n0.9099938\n0.3553888\n0.4787126\n0.00\n0.000\n0.000\n1.00\n1.00\n▇▁▁▁▅\n\n\nnumeric\nPCGNP\n443\n0.8625078\n3591.6509536\n5698.3554010\n52.00\n390.000\n1112.000\n3510.00\n36670.00\n▇▁▁▁▁\n\n\nnumeric\nAINEW\n468\n0.8547486\n2.4433551\n1.1558005\n1.00\n1.000\n2.000\n3.00\n5.00\n▇▇▇▃▂\n\n\nnumeric\nSDNEW\n468\n0.8547486\n2.2618010\n1.1365604\n1.00\n1.000\n2.000\n3.00\n5.00\n▇▇▆▂▁\n\n\nnumeric\nIDGURR\n0\n1.0000000\n455.7709497\n246.5201369\n2.00\n290.000\n450.000\n663.00\n990.00\n▆▇▇▇▃\n\n\nnumeric\nAILAG\n644\n0.8001241\n2.4499612\n1.1479673\n1.00\n1.000\n2.000\n3.00\n5.00\n▇▇▇▃▂\n\n\nnumeric\nSDLAG\n644\n0.8001241\n2.2470908\n1.1156632\n1.00\n1.000\n2.000\n3.00\n5.00\n▇▇▆▂▁\n\n\nnumeric\nPERCHPCG\n618\n0.8081937\n4.6138441\n13.2208934\n-95.50\n-2.545\n4.615\n11.76\n128.57\n▁▂▇▁▁\n\n\nnumeric\nPERCHPOP\n293\n0.9090627\n2.1928815\n4.0424128\n-48.45\n0.910\n2.220\n2.94\n126.01\n▁▇▁▁▁\n\n\nnumeric\nLPOP\n115\n0.9643079\n15.4819279\n1.8633316\n11.00\n14.510\n15.590\n16.64\n20.89\n▂▃▇▃▁\n\n\nnumeric\nPCGTHOU\n443\n0.8625078\n3.5916985\n5.6983334\n0.05\n0.390\n1.110\n3.51\n36.67\n▇▁▁▁▁\n\n\nnumeric\nDEMOC3\n793\n0.7538796\n3.6817620\n4.3577178\n0.00\n0.000\n0.000\n9.00\n10.00\n▇▁▁▂▃\n\n\nnumeric\nCWARCOW\n407\n0.8736809\n0.0920071\n0.2890873\n0.00\n0.000\n0.000\n0.00\n1.00\n▇▁▁▁▁\n\n\nnumeric\nIWARCOW2\n380\n0.8820608\n0.0862069\n0.2807187\n0.00\n0.000\n0.000\n0.00\n1.00\n▇▁▁▁▁",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 1: Time, Decomposition, and Summaries"
    ]
  },
  {
    "objectID": "posts/day-1/index.html#an-xtsum-function-for-r",
    "href": "posts/day-1/index.html#an-xtsum-function-for-r",
    "title": "Day 1: Time, Decomposition, and Summaries",
    "section": "An xtsum function for R",
    "text": "An xtsum function for R\nA little function that I wrote up on github.\n\nlibrary(tidyverse)\nlibrary(plm)\nlibrary(kableExtra)\nsource(url(\"https://raw.githubusercontent.com/robertwwalker/DADMStuff/master/xtsum/xtsum.R\"))\n# Be careful with the ID variable, the safest is to make it factor; this can go wildly wrong\nxtsum(IDORIGIN~., data=HR.Data) %&gt;% kable() %&gt;% scroll_box(width=\"80%\", height=\"50%\")\n\n\n\n\n\n\nO.mean\nO.sd\nO.min\nO.max\nO.SumSQ\nO.N\nB.mean\nB.sd\nB.min\nB.max\nB.Units\nB.t.bar\nW.sd\nW.min\nW.max\nW.SumSQ\nWithin.Ovr.Ratio\n\n\n\n\nYEAR\n1984.5\n5.189\n1976\n1993\n86725.5\n3222\n1984.5\n0\n1984.5\n1984.5\n179\n18\n5.189\n-8.5\n8.5\n86725.5\n1\n\n\nAI\n2.753\n1.075\n1\n5\n2497.538\n2161\n2.498\n0.989\n1\n5\n173\n12.491\n0.631\n-2.375\n2.5625\n860.822\n0.345\n\n\nSD\n2.241\n1.13\n1\n5\n3365.455\n2635\n2.241\n1.004\n1\n5\n178\n14.803\n0.624\n-2.666667\n3.0625\n1025.695\n0.305\n\n\nPOLRT\n3.81\n2.223\n1\n7\n14029.94\n2840\n3.78\n1.99\n1\n7\n179\n15.866\n0.925\n-4\n4.777778\n2428.552\n0.173\n\n\nMIL2\n0.273\n0.445\n0\n1\n563.058\n2840\n0.24\n0.377\n0\n1\n179\n15.866\n0.216\n-0.9444444\n0.8888889\n132.778\n0.236\n\n\nLEFT\n0.176\n0.381\n0\n1\n410.983\n2829\n0.157\n0.334\n0\n1\n179\n15.804\n0.157\n-0.8888889\n0.8888889\n69.611\n0.169\n\n\nBRIT\n0.355\n0.479\n0\n1\n671.685\n2932\n0.335\n0.473\n0\n1\n179\n16.38\n0\n0\n0\n0\n0\n\n\nPCGNP\n3591.651\n5698.355\n52\n36670\n90205144379\n2779\n3449.178\n5049.297\n112.2222\n22653.89\n173\n16.064\n2278.412\n-12303.33\n16961.67\n14421042273\n0.16\n\n\nAINEW\n2.443\n1.156\n1\n5\n3677.663\n2754\n2.379\n1.012\n1\n5\n178\n15.472\n0.622\n-2.388889\n2.944444\n1064.102\n0.289\n\n\nSDNEW\n2.262\n1.137\n1\n5\n3556.241\n2754\n2.253\n1.006\n1\n5\n178\n15.472\n0.631\n-2.588235\n3\n1096.442\n0.308\n\n\nIDGURR\n455.771\n246.52\n2\n990\n195747185\n3222\n455.771\n247.173\n2\n990\n179\n18\n0\n0\n0\n0\n0\n\n\nAILAG\n2.45\n1.148\n1\n5\n3396.045\n2578\n2.402\n1.039\n1\n5\n177\n14.565\n0.609\n-2.411765\n3\n955.37\n0.281\n\n\nSDLAG\n2.247\n1.116\n1\n5\n3207.603\n2578\n2.236\n0.991\n1\n5\n177\n14.565\n0.608\n-2.5\n3.058824\n952.174\n0.297\n\n\nPERCHPCG\n4.614\n13.221\n-95.5\n128.57\n454983.6\n2604\n3.325\n6.893\n-36.21333\n15.03765\n168\n15.5\n12.393\n-92.50235\n114.8882\n399763\n0.879\n\n\nPERCHPOP\n2.193\n4.042\n-48.45\n126.01\n47846.75\n2929\n2.842\n9.443\n-2.126471\n126.01\n176\n16.642\n3.018\n-48.12235\n80.69765\n26663.59\n0.557\n\n\nLPOP\n15.482\n1.863\n11\n20.89\n10784.05\n3107\n15.488\n1.844\n11.09056\n20.76889\n177\n17.554\n0.129\n-0.7288889\n0.7311111\n51.883\n0.005\n\n\nPCGTHOU\n3.592\n5.698\n0.05\n36.67\n90204.45\n2779\n3.449\n5.049\n0.1122222\n22.65389\n173\n16.064\n2.278\n-12.30333\n16.96167\n14420.95\n0.16\n\n\nDEMOC3\n3.682\n4.358\n0\n10\n46107\n2429\n3.774\n3.96\n0\n10\n155\n15.671\n1.726\n-7.277778\n7.941176\n7229.815\n0.157\n\n\nCWARCOW\n0.092\n0.289\n0\n1\n235.17\n2815\n0.095\n0.245\n0\n1\n179\n15.726\n0.175\n-0.8888889\n0.9444444\n85.693\n0.364\n\n\nIWARCOW2\n0.086\n0.281\n0\n1\n223.879\n2842\n0.092\n0.227\n0\n1\n179\n15.877\n0.19\n-0.8888889\n0.9444444\n102.992\n0.46\n\n\n\n\n\n\nSince I wrote this, a formal package was submitted in 2023; I would suggest using it because of passing checks. The package is xtsum.\n\nxtsum::xtsum(HR.Data, id = \"IDORIGIN\", t=\"YEAR\", na.rm = TRUE)\n\n\n\n\nVariable\nDim\nMean\nSD\nMin\nMax\nObservations\n\n\n\n\n___________\n_________\n\n\n\n\n\n\n\nAI\noverall\n2.753\n1.075\n1\n5\nN = 2161\n\n\n\nbetween\n\n0.989\n1\n5\nn = 173\n\n\n\nwithin\n\n0.631\n0.378\n5.316\nT = 12.491\n\n\n___________\n_________\n\n\n\n\n\n\n\nSD\noverall\n2.241\n1.13\n1\n5\nN = 2635\n\n\n\nbetween\n\n1.004\n1\n5\nn = 178\n\n\n\nwithin\n\n0.624\n-0.426\n5.303\nT = 14.803\n\n\n___________\n_________\n\n\n\n\n\n\n\nPOLRT\noverall\n3.81\n2.223\n1\n7\nN = 2840\n\n\n\nbetween\n\n1.99\n1\n7\nn = 179\n\n\n\nwithin\n\n0.925\n-0.19\n8.587\nT = 15.866\n\n\n___________\n_________\n\n\n\n\n\n\n\nMIL2\noverall\n0.273\n0.445\n0\n1\nN = 2840\n\n\n\nbetween\n\n0.377\n0\n1\nn = 179\n\n\n\nwithin\n\n0.216\n-0.672\n1.161\nT = 15.866\n\n\n___________\n_________\n\n\n\n\n\n\n\nLEFT\noverall\n0.176\n0.381\n0\n1\nN = 2829\n\n\n\nbetween\n\n0.334\n0\n1\nn = 179\n\n\n\nwithin\n\n0.157\n-0.713\n1.065\nT = 15.804\n\n\n___________\n_________\n\n\n\n\n\n\n\nBRIT\noverall\n0.355\n0.479\n0\n1\nN = 2932\n\n\n\nbetween\n\n0.473\n0\n1\nn = 179\n\n\n\nwithin\n\n0\n0.355\n0.355\nT = 16.38\n\n\n___________\n_________\n\n\n\n\n\n\n\nPCGNP\noverall\n3591.651\n5698.355\n52\n36670\nN = 2779\n\n\n\nbetween\n\n5049.297\n112.222\n22653.889\nn = 173\n\n\n\nwithin\n\n2278.412\n-8711.682\n20553.318\nT = 16.064\n\n\n___________\n_________\n\n\n\n\n\n\n\nAINEW\noverall\n2.443\n1.156\n1\n5\nN = 2754\n\n\n\nbetween\n\n1.012\n1\n5\nn = 178\n\n\n\nwithin\n\n0.622\n0.054\n5.388\nT = 15.472\n\n\n___________\n_________\n\n\n\n\n\n\n\nSDNEW\noverall\n2.262\n1.137\n1\n5\nN = 2754\n\n\n\nbetween\n\n1.006\n1\n5\nn = 178\n\n\n\nwithin\n\n0.631\n-0.326\n5.262\nT = 15.472\n\n\n___________\n_________\n\n\n\n\n\n\n\nIDGURR\noverall\n455.771\n246.52\n2\n990\nN = 3222\n\n\n\nbetween\n\n247.173\n2\n990\nn = 179\n\n\n\nwithin\n\n0\n455.771\n455.771\nT = 18\n\n\n___________\n_________\n\n\n\n\n\n\n\nAILAG\noverall\n2.45\n1.148\n1\n5\nN = 2578\n\n\n\nbetween\n\n1.039\n1\n5\nn = 177\n\n\n\nwithin\n\n0.609\n0.038\n5.45\nT = 14.565\n\n\n___________\n_________\n\n\n\n\n\n\n\nSDLAG\noverall\n2.247\n1.116\n1\n5\nN = 2578\n\n\n\nbetween\n\n0.991\n1\n5\nn = 177\n\n\n\nwithin\n\n0.608\n-0.253\n5.306\nT = 14.565\n\n\n___________\n_________\n\n\n\n\n\n\n\nPERCHPCG\noverall\n4.614\n13.221\n-95.5\n128.57\nN = 2604\n\n\n\nbetween\n\n6.893\n-36.213\n15.038\nn = 168\n\n\n\nwithin\n\n12.393\n-87.889\n119.502\nT = 15.5\n\n\n___________\n_________\n\n\n\n\n\n\n\nPERCHPOP\noverall\n2.193\n4.042\n-48.45\n126.01\nN = 2929\n\n\n\nbetween\n\n9.443\n-2.126\n126.01\nn = 176\n\n\n\nwithin\n\n3.018\n-45.929\n82.891\nT = 16.642\n\n\n___________\n_________\n\n\n\n\n\n\n\nLPOP\noverall\n15.482\n1.863\n11\n20.89\nN = 3107\n\n\n\nbetween\n\n1.844\n11.091\n20.769\nn = 177\n\n\n\nwithin\n\n0.129\n14.753\n16.213\nT = 17.554\n\n\n___________\n_________\n\n\n\n\n\n\n\nPCGTHOU\noverall\n3.592\n5.698\n0.05\n36.67\nN = 2779\n\n\n\nbetween\n\n5.049\n0.112\n22.654\nn = 173\n\n\n\nwithin\n\n2.278\n-8.712\n20.553\nT = 16.064\n\n\n___________\n_________\n\n\n\n\n\n\n\nDEMOC3\noverall\n3.682\n4.358\n0\n10\nN = 2429\n\n\n\nbetween\n\n3.96\n0\n10\nn = 155\n\n\n\nwithin\n\n1.726\n-3.596\n11.623\nT = 15.671\n\n\n___________\n_________\n\n\n\n\n\n\n\nCWARCOW\noverall\n0.092\n0.289\n0\n1\nN = 2815\n\n\n\nbetween\n\n0.245\n0\n1\nn = 179\n\n\n\nwithin\n\n0.175\n-0.797\n1.036\nT = 15.726\n\n\n___________\n_________\n\n\n\n\n\n\n\nIWARCOW2\noverall\n0.086\n0.281\n0\n1\nN = 2842\n\n\n\nbetween\n\n0.227\n0\n1\nn = 179\n\n\n\nwithin\n\n0.19\n-0.803\n1.031\nT = 15.877",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 1: Time, Decomposition, and Summaries"
    ]
  },
  {
    "objectID": "posts/day-1/index.html#the-core-idea",
    "href": "posts/day-1/index.html#the-core-idea",
    "title": "Day 1: Time, Decomposition, and Summaries",
    "section": "The Core Idea",
    "text": "The Core Idea\nIn R, this is an essential group_by calculation in the tidyverse. The between data are a summary table with units constituting the rows. The within data is the overall data with group means subtracted.\n\nHR.Data %&gt;% \n  group_by(IDORIGIN) %&gt;% \n  mutate(DEMOC.Centered = \n           DEMOC3 - mean(DEMOC3, na.rm=TRUE)) %&gt;%\n  filter(IDORIGIN==42) %&gt;% \n  select(IDORIGIN, YEAR, DEMOC3, DEMOC.Centered) \n\n# A tibble: 18 × 4\n# Groups:   IDORIGIN [1]\n   IDORIGIN  YEAR DEMOC3 DEMOC.Centered\n      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n 1       42  1976      1         -5.11 \n 2       42  1977      1         -5.11 \n 3       42  1978      6         -0.111\n 4       42  1979      6         -0.111\n 5       42  1980      6         -0.111\n 6       42  1981      6         -0.111\n 7       42  1982      7          0.889\n 8       42  1983      7          0.889\n 9       42  1984      7          0.889\n10       42  1985      7          0.889\n11       42  1986      7          0.889\n12       42  1987      7          0.889\n13       42  1988      7          0.889\n14       42  1989      7          0.889\n15       42  1990      7          0.889\n16       42  1991      7          0.889\n17       42  1992      7          0.889\n18       42  1993      7          0.889\n\n\nIn Stata, the key is to first load and declare the data.\nuse \"https://github.com/robertwwalker/Essex-Data/raw/main/ISQ99-Essex.dta\"\ndes\n\n\n\nStata Load\n\n\nxtset denotes two key features of the data, the \\(i\\) and \\(t\\).\nxtset IDORIGIN YEAR\n\n\n\nxtset\n\n\nStata has internal capabilities for summarising and describing xt data. A between and within summary is given by xtsum\nxtsum\n\n\n\nxtsum\n\n\nThe description can be deceptive because the indices are a complete grid.\nxtdes\n\n\n\nxtdes",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 1: Time, Decomposition, and Summaries"
    ]
  },
  {
    "objectID": "posts/day-1/index.html#qualitative-variables",
    "href": "posts/day-1/index.html#qualitative-variables",
    "title": "Day 1: Time, Decomposition, and Summaries",
    "section": "Qualitative Variables",
    "text": "Qualitative Variables\n\nxttab\n\nBetween: How many units received each category?\nWithin: Of all observations of units that received that category at least once, what percent of observations take this value?\n\nxttab AINEW\n\n\n\nxttab\n\n\n\n\nThe Between\n\nbetween.tally &lt;- function(x) {\nHR.Data %&gt;% select(IDORIGIN, AINEW) %&gt;% filter(AINEW==x) %$% table(IDORIGIN) %&gt;% length()\n}\nsapply(c(1:5), function(x) {between.tally(x)})\n\n[1]  96 121 113  86  43\n\n\n\n\nxttrans\nA first-order transition matrix.\nxttrans AINEW\n I want to rely on the dplyr version of lag so I am explicit here. Take the data, group it by id, calculate the lag, ungroup them, and create a table. I prefer to keep this explicit with order_by. The janitor library provides tabyl and it is explicit among missing values.\n\nlibrary(janitor)\nHR.Data %&gt;% \n  group_by(IDORIGIN) %&gt;% \n  mutate(Lag.AI = dplyr::lag(AINEW, n=1L, order_by = YEAR)) %&gt;%\n  ungroup() %&gt;%\n  tabyl(Lag.AI, AINEW)\n\n Lag.AI   1   2   3   4  5 NA_\n      1 545  83  13   1  0   5\n      2  86 480 130  20  3   2\n      3   5 150 496  76 14   7\n      4   2  14  97 167 47   0\n      5   0   1   5  53 76   0\n     NA  67  41  41  33  8 454",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 1: Time, Decomposition, and Summaries"
    ]
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I hope you are ready!\nIn the next two weeks we will set forth as comprehensive a treatment as we can in time series and panel data in ten sessions. With coffee in hand and the computer getting ready, let’s dig in!\n\nA brief summary for the Instructor’s Presentation\n\nWeek 1: Time Series Day 1\n\nStationarity and Unit Roots\nARMA structures\nDynamic Linear Models and their Interpretation\nVAR and Cointegration topics\nEquation balance and recent controversies\n\nWeek 2: Panel Data\n\nFixed and random effects and Within/Between\nEstimating Models, xtmixed, xtreg, etc.\nSTADL Up!\nMissing data\nGLM Extensions of Panel Models\nDynamic Panel Data\nCausal Inference in Panel Data: TWFE/DID",
    "crumbs": [
      "Welcome!",
      "Welcome to 2W!"
    ]
  },
  {
    "objectID": "posts/welcome/index.html#a-useful-model-translator",
    "href": "posts/welcome/index.html#a-useful-model-translator",
    "title": "Welcome!",
    "section": "A Useful Model Translator",
    "text": "A Useful Model Translator\nThe Social Science Computing Cooperative at the University of Wisconsin has a nice, albeit incomplete, rendition of model compatibilities with syntax in R and Stata.",
    "crumbs": [
      "Welcome!",
      "Welcome to 2W!"
    ]
  },
  {
    "objectID": "posts/welcome/index.html#topics-of-interest-cut-by-time",
    "href": "posts/welcome/index.html#topics-of-interest-cut-by-time",
    "title": "Welcome!",
    "section": "Topics of Interest Cut by Time",
    "text": "Topics of Interest Cut by Time\nWe will read selections from a broader controversy in Political Analysis on Time Series and error correction models; the full list of papers can be found here as a large part of the Winter 2016 issue 24(1).\nThere is a digital special issue of Political Analysis on Regression Discontinuity and Time Series Cross-Section data. For day 6 of the class, the paper by Plumper and Troeger (2019) about fixed effects regression is definitely worth considering.\nFactor augmented dynamic panel data models get an interesting treatment in Dynamic Panel Analysis under Cross-Sectional Dependence by Khusrav Gaibulloev, Todd Sandler and Donggyu Sul",
    "crumbs": [
      "Welcome!",
      "Welcome to 2W!"
    ]
  },
  {
    "objectID": "posts/day-3/index.html",
    "href": "posts/day-3/index.html",
    "title": "Day 3: Dynamic Linear, ADL Models, and VARs",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 3: Dynamic Linear, ADL Models, and VARs"
    ]
  },
  {
    "objectID": "posts/day-3/index.html#slides",
    "href": "posts/day-3/index.html#slides",
    "title": "Day 3: Dynamic Linear, ADL Models, and VARs",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 3: Dynamic Linear, ADL Models, and VARs"
    ]
  },
  {
    "objectID": "posts/day-3/index.html#dynamic-regression",
    "href": "posts/day-3/index.html#dynamic-regression",
    "title": "Day 3: Dynamic Linear, ADL Models, and VARs",
    "section": "Dynamic Regression",
    "text": "Dynamic Regression\nRegression techniques can be reexamined through the lens of dynamic linear models and systems of linear equations. Whether autoregressive distributed lag models or VAR systems and structural time series, regression approaches to time series almost inevitably culminate in thinking about causation as conceived by Clive Granger – Granger causality.\nToday we cover chapters 3 and 4 in TSASS spanning dynamic regression models and dynamic systems. We will start with dynamic regression models by detailing how they are specified and interpreted with distributed lag and autoregressive distributed lag [ADL] models. We also examine the crucial issue of consistency before turning to structural time series models for multiple equations and their counterparts – VARs.",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 3: Dynamic Linear, ADL Models, and VARs"
    ]
  },
  {
    "objectID": "posts/day-3/index.html#vars",
    "href": "posts/day-3/index.html#vars",
    "title": "Day 3: Dynamic Linear, ADL Models, and VARs",
    "section": "VARs",
    "text": "VARs\nWe will start with some data on lung deaths from Australia.\nlibrary(forecast)\nmdeaths\nfdeaths\nsave(mdeaths, fdeaths, file = \"./img/LungDeaths.RData\")\nuse \"https://github.com/robertwwalker/Essex-Data/raw/main/Lung-Deaths.dta\"\n\nlibrary(hrbrthemes); library(fpp3)\nload(url(\"https://github.com/robertwwalker/Essex-Data/raw/main/LungDeaths.RData\"))\nMales &lt;- mdeaths; Females &lt;- fdeaths\nLung.Deaths &lt;- cbind(Males, Females) %&gt;% as_tsibble()\nLung.Deaths %&gt;% autoplot() + theme_ipsum_rc() + labs(y=\"Lung Deaths\", x=\"Month [1M]\", title=\"Lung Deaths among Males and Females\") + guides(color=\"none\")\n\n\n\n\n\n\n\n\n\nlung_deaths &lt;- cbind(mdeaths, fdeaths) %&gt;%\n  as_tsibble(pivot_longer = FALSE)\nlung_deaths &lt;- cbind(mdeaths, fdeaths) %&gt;%\n  as_tsibble(pivot_longer = FALSE)\nfit &lt;- lung_deaths %&gt;%\n  model(VAR(vars(mdeaths, fdeaths) ~ AR(3)))\nreport(fit)\n\nSeries: mdeaths, fdeaths \nModel: VAR(3) w/ mean \n\nCoefficients for mdeaths:\n      lag(mdeaths,1)  lag(fdeaths,1)  lag(mdeaths,2)  lag(fdeaths,2)\n              0.6675          0.8074          0.3677         -1.4540\ns.e.          0.3550          0.8347          0.3525          0.8088\n      lag(mdeaths,3)  lag(fdeaths,3)  constant\n              0.2606         -1.1214  538.7817\ns.e.          0.3424          0.8143  137.1047\n\nCoefficients for fdeaths:\n      lag(mdeaths,1)  lag(fdeaths,1)  lag(mdeaths,2)  lag(fdeaths,2)\n              0.2138          0.4563          0.0937         -0.3984\ns.e.          0.1460          0.3434          0.1450          0.3328\n      lag(mdeaths,3)  lag(fdeaths,3)  constant\n              0.0250          -0.315  202.0027\ns.e.          0.1409           0.335   56.4065\n\nResidual covariance matrix:\n         mdeaths  fdeaths\nmdeaths 58985.95 22747.94\nfdeaths 22747.94  9983.95\n\nlog likelihood = -812.35\nAIC = 1660.69   AICc = 1674.37  BIC = 1700.9\n\n\nvar mdeaths fdeaths, lags(1 2 3)  \n\nfit2 &lt;- lung_deaths %&gt;%\n  model(VAR(vars(mdeaths, fdeaths) ~ AR(2)))\nreport(fit2)\n\nSeries: mdeaths, fdeaths \nModel: VAR(2) w/ mean \n\nCoefficients for mdeaths:\n      lag(mdeaths,1)  lag(fdeaths,1)  lag(mdeaths,2)  lag(fdeaths,2)  constant\n              0.9610          0.3340          0.1149         -1.3379  443.8492\ns.e.          0.3409          0.8252          0.3410          0.7922  124.4608\n\nCoefficients for fdeaths:\n      lag(mdeaths,1)  lag(fdeaths,1)  lag(mdeaths,2)  lag(fdeaths,2)  constant\n              0.3391          0.2617         -0.0601         -0.2691  145.0546\ns.e.          0.1450          0.3510          0.1450          0.3369   52.9324\n\nResidual covariance matrix:\n         mdeaths  fdeaths\nmdeaths 62599.51 24942.79\nfdeaths 24942.79 11322.70\n\nlog likelihood = -833.17\nAIC = 1694.35   AICc = 1701.98  BIC = 1725.83\n\n\nvar mdeaths fdeaths, lags(1 2)  \nGranger causation wants a non-tidy format. I will use the conventional VAR syntax from vars that wants the collection of endogenous variables as inputs by themselves in a matrix form. We can also specify exogenous variable for such systems with their data matrix in the argument exogen=....\n\nlibrary(bruceR)\nVarmf &lt;- vars::VAR(lung_deaths[,c(\"mdeaths\",\"fdeaths\")], p=3, type=\"const\")\ngranger_causality(Varmf)\n\n\nGranger Causality Test (Multivariate)\n\nF test and Wald χ² test based on VAR(3) model:\n────────────────────────────────────────────────────────────────\n                          F df1 df2     p     Chisq df     p    \n────────────────────────────────────────────────────────────────\n ------------------                                             \n mdeaths &lt;= fdeaths    1.93   3  62  .134      5.80  3  .122    \n mdeaths &lt;= ALL        1.93   3  62  .134      5.80  3  .122    \n ------------------                                             \n fdeaths &lt;= mdeaths    1.20   3  62  .316      3.61  3  .307    \n fdeaths &lt;= ALL        1.20   3  62  .316      3.61  3  .307    \n────────────────────────────────────────────────────────────────\n\n\nvargranger\n\nfit %&gt;%\n  fabletools::forecast(h=12) %&gt;%\n  autoplot(lung_deaths)\n\n\n\n\n\n\n\n\n\nFemale\n\nlung_deaths %&gt;%\nmodel(VAR(vars(mdeaths, fdeaths) ~ AR(3))) %&gt;%\n  residuals() %&gt;% \n  pivot_longer(., cols = c(mdeaths,fdeaths)) %&gt;% \n  filter(name==\"fdeaths\") %&gt;% \n  as_tsibble(index=index) %&gt;% \n  gg_tsdisplay(plot_type = \"partial\") + labs(title=\"Female residuals\")\n\n\n\n\n\n\n\n\n\n\nMale\n\nlung_deaths %&gt;%\nmodel(VAR(vars(mdeaths, fdeaths) ~ AR(3))) %&gt;%\n  residuals() %&gt;% \n  pivot_longer(., cols = c(mdeaths,fdeaths)) %&gt;% \n  filter(name==\"mdeaths\") %&gt;% \n  as_tsibble(index=index) %&gt;% \n  gg_tsdisplay(plot_type = \"partial\") + labs(title=\"Male residuals\")",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 3: Dynamic Linear, ADL Models, and VARs"
    ]
  },
  {
    "objectID": "posts/day-3/index.html#easy-impulse-response",
    "href": "posts/day-3/index.html#easy-impulse-response",
    "title": "Day 3: Dynamic Linear, ADL Models, and VARs",
    "section": "Easy Impulse Response",
    "text": "Easy Impulse Response\nWhat happens if I shock one of the series; how does it work through the system?\nThe idea behind an impulse-response is core to counterfactual analysis with time series. What does our future world look like and what predictions arise from it and the model we have deployed?\nWhether VARs or dynamic linear models or ADL models, these are key to interpreting a model in the real world.\n\nMales\nirf set \"M:\\t1.irf\"\nirf create irf1, order( mdeaths fdeaths)\nirf graph irf\n\nVARMF &lt;- cbind(Males,Females)\nmod1 &lt;- vars::VAR(VARMF, p=3, type=\"const\")\nplot(vars::irf(mod1, boot=TRUE, impulse=\"Males\"))\n\n\n\n\n\n\n\n\n\n\nFemales\nirf set \"M:\\tF.irf\"\nirf create Female, order( fdeaths mdeaths)\nirf graph irf\n\nplot(vars::irf(mod1, boot=TRUE, impulse=\"Females\"))",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 3: Dynamic Linear, ADL Models, and VARs"
    ]
  },
  {
    "objectID": "posts/day-5/index.html",
    "href": "posts/day-5/index.html",
    "title": "Day 5: Equation Balance to Wrap Up Week 1",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 5: Equation Balance to Wrap Up Week 1"
    ]
  },
  {
    "objectID": "posts/day-5/index.html#slides",
    "href": "posts/day-5/index.html#slides",
    "title": "Day 5: Equation Balance to Wrap Up Week 1",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 5: Equation Balance to Wrap Up Week 1"
    ]
  },
  {
    "objectID": "posts/day-5/index.html#equation-balance",
    "href": "posts/day-5/index.html#equation-balance",
    "title": "Day 5: Equation Balance to Wrap Up Week 1",
    "section": "Equation Balance",
    "text": "Equation Balance\nThe article by Pickup and Kellstedt forms the basis for summary remarks on time series and a useful transition into the study of multiple time series formally for week two.\nThe lingering issue that does not get adequate treatment owing to time is fractional integration methods. Indeed, the article by Lebo and Grant (2016) summarising the issues in the 2016 special issue is worth digesting.",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 5: Equation Balance to Wrap Up Week 1"
    ]
  },
  {
    "objectID": "posts/day-2/index.html",
    "href": "posts/day-2/index.html",
    "title": "Day 2: Time Series, Stationarity, and ARIMA Models",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 2: Time Series, Stationarity, and ARIMA Models"
    ]
  },
  {
    "objectID": "posts/day-2/index.html#slides",
    "href": "posts/day-2/index.html#slides",
    "title": "Day 2: Time Series, Stationarity, and ARIMA Models",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 2: Time Series, Stationarity, and ARIMA Models"
    ]
  },
  {
    "objectID": "posts/day-2/index.html#simulating-arima-processes",
    "href": "posts/day-2/index.html#simulating-arima-processes",
    "title": "Day 2: Time Series, Stationarity, and ARIMA Models",
    "section": "Simulating ARIMA processes",
    "text": "Simulating ARIMA processes\nWe want to simulate data under an ARIMA (p, d, q) model. arima.sim wants inputs as a list where the expected length of the ar and ma vectors that will hold the actual values of the ar and ma parameters. Here, I ask for a series that is I(1) with a first-order ar=0.1 and a first-order ma=-0.5. Let me start by generating it and plotting the time series.\n\nlibrary(fpp3)\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n\n── Attaching packages ──────────────────────────────────────────── fpp3 1.0.1 ──\n\n\n✔ tibble      3.2.1     ✔ tsibble     1.1.6\n✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n✔ tidyr       1.3.1     ✔ feasts      0.4.1\n✔ lubridate   1.9.3     ✔ fable       0.4.1\n✔ ggplot2     3.5.1     \n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\nn &lt;- 100\nmy.data &lt;- data.frame(\n  x=arima.sim(n = n, \n              model=list(order = c(1, 1, 1), ar=c(0.7), ma=c(-0.5)), n.start=20), \n  dtime = seq(1,n+1))\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nmy.data %&lt;&gt;% as_tsibble(index=dtime) \nmy.data %&gt;% autoplot() + labs(title=\"A (1, 1, 1) Series\", x=\"Time\")\n\nPlot variable not specified, automatically selected `.vars = x`\n\n\n\n\n\n\n\n\n\nNow I want to display the ACF and PACF in levels.\n\nlibrary(patchwork)\n{my.data %&gt;% ACF(x, lag_max=20) %&gt;% \n    autoplot() } + \n  {my.data %&gt;% PACF(x, lag_max=20) %&gt;% \n      autoplot() }\n\n\n\n\n\n\n\n\nFinally, let me display the ACF and PACF with differenced data.\n\n{my.data %&gt;% ACF(diff(x), lag_max=20) %&gt;% \n    autoplot() } + \n  {my.data %&gt;% PACF(diff(x), lag_max=20) %&gt;% \n      autoplot() }",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 2: Time Series, Stationarity, and ARIMA Models"
    ]
  },
  {
    "objectID": "posts/day-2/index.html#nonsense-regressions-of-i1-series",
    "href": "posts/day-2/index.html#nonsense-regressions-of-i1-series",
    "title": "Day 2: Time Series, Stationarity, and ARIMA Models",
    "section": "Nonsense Regressions of I(1) Series",
    "text": "Nonsense Regressions of I(1) Series",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 2: Time Series, Stationarity, and ARIMA Models"
    ]
  },
  {
    "objectID": "posts/day-2/index.html#an-example-of-time-series-troubles",
    "href": "posts/day-2/index.html#an-example-of-time-series-troubles",
    "title": "Day 2: Time Series, Stationarity, and ARIMA Models",
    "section": "An Example of Time Series Troubles",
    "text": "An Example of Time Series Troubles\nLet me do this with a relatively simple regression. Two variables:\n\\[ y = \\alpha + \\beta x + \\epsilon \\]\nBoth are generated randomly. Here’s a basic plot.\n\ny &lt;- cumsum(rnorm(100))\nx &lt;- cumsum(rnorm(100))\nplot(x=seq(1:100), y=y, type=\"l\", col=\"red\", ylim=c(-15,15))\nlines(x=seq(1:100), y=x, col=\"blue\")\n\n\n\n\n\n\n\n\nEach time series contains 100 observations. Because both x and y are random, the slopes should be 0, 95% of the time with 95% confidence because there is no underlying relationship. In practice, let’s look at the distribution of p-values for the probability of no relationship.\n\nSR &lt;- function(n) {\n  Results &lt;- NULL\n  for(i in 1:n) {\ny &lt;- cumsum(rnorm(100))\nx &lt;- cumsum(rnorm(100))\nResult &lt;- summary(lm(y~x))$coefficients[2,4]\nResults &lt;- append(Result,Results)\n  }\n  Results\n}\n\nI replicate the process of random x and random y 1000 times and show the p-values below. Because they are random, approximately 95% should be greater than 0.05.\n\nRes1 &lt;- SR(1000)\nplot(density(Res1), main=\"Distribution of p-values from Trending x\")\n\n\n\n\n\n\n\n\nIn practice,\n\ntable(Res1 &gt; 0.05)\n\n\nFALSE  TRUE \n  774   226 \n\n\nThe above table should show about 950 TRUE and 50 FALSE but because each is trended and they share variation from trend, the actual frequency of rejecting the claim of no relationship is far more common than 5%.",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 2: Time Series, Stationarity, and ARIMA Models"
    ]
  },
  {
    "objectID": "posts/day-2/index.html#arima-models-with-government-popularity",
    "href": "posts/day-2/index.html#arima-models-with-government-popularity",
    "title": "Day 2: Time Series, Stationarity, and ARIMA Models",
    "section": "ARIMA Models with Government Popularity",
    "text": "ARIMA Models with Government Popularity\n\nlibrary(fpp3)\nlibrary(haven)\nbr7983 &lt;- read_dta(url(\"https://github.com/robertwwalker/Essex-Data/raw/main/br7983.dta\")) %&gt;% \n  mutate(month = as.character(month)) %&gt;% \n  mutate(month = paste0(\"19\",month, sep=\"\")) %&gt;% \n  mutate(date = yearmonth(month, format=\"%Y%m\"))\nbr7983 &lt;- br7983 %&gt;% as_tsibble(index=date) \nbr7983 %&gt;% autoplot(govpopl) + hrbrthemes::theme_ipsum() + labs(y=\"logged Government Popularity\")",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 2: Time Series, Stationarity, and ARIMA Models"
    ]
  },
  {
    "objectID": "posts/day-2/index.html#time-series-features",
    "href": "posts/day-2/index.html#time-series-features",
    "title": "Day 2: Time Series, Stationarity, and ARIMA Models",
    "section": "Time Series Features",
    "text": "Time Series Features\n\nbr7983 %&gt;% gg_tsdisplay(govpopl, plot_type = \"partial\")\n\n\n\n\n\n\n\n\nlibrary(haven)\n# To install TSA, it works in three steps.\n# Link to package\n# https://cran.r-project.org/web/packages/TSA/index.html\n# The archive for the package is:\n# https://cran.r-project.org/src/contrib/Archive/TSA/\n# I grabbed the most recent one.\n# Then I used the RStudio: Tools &gt; Install Packages &gt; From a local archive\n# And installed it.\n# It had dependency chains to fix.\n# Those can be fixed with\n# install.packages(c(\"leaps\", \"locfit\", \"mgcv\"))\n\nlibrary(TSA)\n# Replicating the abrupt permanent in April\narimax(br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), xreg=br7983$flandd)\n\n\nCall:\narimax(x = br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), \n    xreg = br7983$flandd)\n\nCoefficients:\n        sma1  intercept    xreg\n      0.3430     0.0015  0.0687\ns.e.  0.1347     0.0184  0.0950\n\nsigma^2 estimated as 0.009071:  log likelihood = 43.57,  aic = -81.15\n\n\n\n# Replicating the abrupt permanent in May\narimax(br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), xreg=br7983$flanddlag1)\n\n\nCall:\narimax(x = br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), \n    xreg = br7983$flanddlag1)\n\nCoefficients:\n        sma1  intercept    xreg\n      0.2668    -0.0033  0.3124\ns.e.  0.1310     0.0153  0.0842\n\nsigma^2 estimated as 0.007019:  log likelihood = 49.7,  aic = -93.41\n\n\n\n# Replicating the gradual permanent April\narimax(br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), xtransf = br7983$flandd, transfer = list(c(1,0)))\n\n\nCall:\narimax(x = br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), \n    xtransf = br7983$flandd, transfer = list(c(1, 0)))\n\nCoefficients:\n        sma1  intercept  T1-AR1  T1-MA0\n      0.3930    -0.0080  0.6479  0.1885\ns.e.  0.1251     0.0185  0.1528  0.0718\n\nsigma^2 estimated as 0.007946:  log likelihood = 46.6,  aic = -85.2\n\n\n\n# Replicating the gradual permanent May\n# Does not work; degrees of freedom?\n# arimax(br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), xtransf = br7983$flanddlag1, transfer = list(c(1,0)))\n# Falklands - gradual temporary (pulse decay) effect - May 1982\n# Does not work; degrees of freedom?\n# arimax(br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), xtransf = br7983$flanddlag1, transfer = list(c(1,1)))\n# These are fairly demanding [of the data] models.\narimax(br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), xtransf = br7983$flandd, transfer = list(c(1,0)))\n\n\nCall:\narimax(x = br7983$govpopld, seasonal = list(order = c(0, 0, 1), period = 4), \n    xtransf = br7983$flandd, transfer = list(c(1, 0)))\n\nCoefficients:\n        sma1  intercept  T1-AR1  T1-MA0\n      0.3930    -0.0080  0.6479  0.1885\ns.e.  0.1251     0.0185  0.1528  0.0718\n\nsigma^2 estimated as 0.007946:  log likelihood = 46.6,  aic = -85.2\n\n\n\nARIMA\nStata covers this in the following link.\nwpi1 &lt;- read_stata(url(\"http://www.stata-press.com/data/r12/wpi1.dta\"))\nwpi1$date &lt;- yearquarter(wpi1$t, fiscal_start = 1)-40\n\nload(url(\"https://github.com/robertwwalker/Essex-Data/raw/main/wpi1.RData\"))\nwpi1 %&gt;% as_tsibble(index=date) %&gt;% gg_tsdisplay(ln_wpi, plot_type = \"partial\") + labs(title=\"Log WPI\")\n\n\n\n\n\n\n\n\n\n\nStata\narima wpi, arima(1,1,1)\n\n\nR\n\nwpi1 %&gt;% as_tsibble(index=date) %&gt;% \n  model(arima = ARIMA(wpi ~ 1 + pdq(1,1,1) + PDQ(0,0,0))) %&gt;% \n  report()\n\nSeries: wpi \nModel: ARIMA(1,1,1) w/ drift \n\nCoefficients:\n         ar1      ma1  constant\n      0.8742  -0.4120    0.0943\ns.e.  0.0637   0.1221    0.0367\n\nsigma^2 estimated as 0.5388:  log likelihood=-135.35\nAIC=278.7   AICc=279.04   BIC=289.95\n\n\nThe help for ARIMA explains the alternative parameterization.\n\n# Using stats::arima\narima(diff(wpi1$wpi), order=c(1,0,1), include.mean = TRUE)\n\n\nCall:\narima(x = diff(wpi1$wpi), order = c(1, 0, 1), include.mean = TRUE)\n\nCoefficients:\n         ar1      ma1  intercept\n      0.8742  -0.4120     0.7499\ns.e.  0.0637   0.1221     0.2921\n\nsigma^2 estimated as 0.5257:  log likelihood = -135.35,  aic = 276.7",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 2: Time Series, Stationarity, and ARIMA Models"
    ]
  },
  {
    "objectID": "posts/day-2/index.html#seasonal",
    "href": "posts/day-2/index.html#seasonal",
    "title": "Day 2: Time Series, Stationarity, and ARIMA Models",
    "section": "Seasonal",
    "text": "Seasonal\narima D.ln_wpi, ar(1) ma(1 4)\n\nwpi1 %&gt;% as_tsibble(index=date) %&gt;% \n  model(arima = ARIMA(ln_wpi ~ 1 + pdq(1,1,1) + PDQ(0,0,1))) %&gt;% \n  report()\n\nSeries: ln_wpi \nModel: ARIMA(1,1,1)(0,0,1)[4] w/ drift \n\nCoefficients:\n         ar1      ma1    sma1  constant\n      0.8289  -0.4252  0.2403    0.0019\ns.e.  0.0854   0.1374  0.0959    0.0007\n\nsigma^2 estimated as 0.0001144:  log likelihood=385.27\nAIC=-760.53   AICc=-760.02   BIC=-746.47\n\n\nThere are also bits about seasonal arima – sarima – and arimax but Stata is fundamentally limited here.\n\nwpi1 %&gt;% as_tsibble(index=date) %&gt;% \n  model(arima = ARIMA(ln_wpi ~ 1 + pdq(1,1,1) + PDQ(0,0,1))) %&gt;% \n  gg_tsresiduals()\n\n\n\n\n\n\n\n\nThat works.",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 2: Time Series, Stationarity, and ARIMA Models"
    ]
  },
  {
    "objectID": "posts/day-4/index.html",
    "href": "posts/day-4/index.html",
    "title": "Day 4: Cointegration",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 4: Cointegration"
    ]
  },
  {
    "objectID": "posts/day-4/index.html#slides",
    "href": "posts/day-4/index.html#slides",
    "title": "Day 4: Cointegration",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 4: Cointegration"
    ]
  },
  {
    "objectID": "posts/day-4/index.html#a-replication",
    "href": "posts/day-4/index.html#a-replication",
    "title": "Day 4: Cointegration",
    "section": "A replication",
    "text": "A replication\nThis is an example with an ECM from a model presented by DeBoef and Keele in their excellent paper “Taking Time Seriously: Dynamic Regression.” The data come from a paper Gilmour and Wolbrecht.\nuse \"https://github.com/robertwwalker/Essex-Data/raw/main/dgw.dta\", clear\n* These data have already been time set:\ntsset\n* The dependent variable in this example is Congressional Approval\n*Table 2\nreg capp l.capp econexp nytavg kg hb vetoes override intrasum mbill \nbgodfrey, lag(1 2 3)\n\nlibrary(haven); library(tidyverse); library(fpp3)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\n── Attaching packages ──────────────────────────────────────────── fpp3 1.0.1 ──\n\n✔ tsibble     1.1.6     ✔ feasts      0.4.1\n✔ tsibbledata 0.4.1     ✔ fable       0.4.1\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\ndgw.data &lt;- read_stata(\"https://github.com/robertwwalker/Essex-Data/raw/main/dgw.dta\")\ndgw.data &lt;- dgw.data %&gt;% filter(!is.na(year))\ndgw.data &lt;- dgw.data %&gt;% mutate(date = paste0(year,\" Q\",quarter, sep=\"\")) %&gt;% mutate(dateQ = yearquarter(date))\ndgw.ts &lt;- dgw.data %&gt;% as_tsibble(index=dateQ)\n# A dynamic linear model\ndgw.ts %&gt;% model(TSLM(capp~lag(capp,1)+econexp+nytavg+kg+hb+vetoes+override+intrasum+mbills)) %&gt;% report()\n\nSeries: capp \nModel: TSLM \n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.1137 -1.3908 -0.1014  1.5446  7.0873 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.37793    3.33043   2.816  0.00634 ** \nlag(capp, 1)  0.79605    0.05166  15.409  &lt; 2e-16 ***\neconexp       0.07173    0.03090   2.321  0.02323 *  \nnytavg        0.20716    0.06982   2.967  0.00413 ** \nkg           -1.29097    1.08646  -1.188  0.23881    \nhb           -4.68564    1.69975  -2.757  0.00746 ** \nvetoes        0.24382    0.08898   2.740  0.00781 ** \noverride     -0.99198    0.55252  -1.795  0.07697 .  \nintrasum     -0.16907    0.12199  -1.386  0.17021    \nmbills       -0.43939    0.28270  -1.554  0.12469    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.976 on 69 degrees of freedom\nMultiple R-squared: 0.885,  Adjusted R-squared: 0.8699\nF-statistic: 58.97 on 9 and 69 DF, p-value: &lt; 2.22e-16\n\n\n*Table 2 with Pres. Approval\nreg capp l.capp p_prap econexp nytavg kg hb vetoes override intrasum mbill \nbgodfrey, lag(1 2 3)\n\ndgw.ts %&gt;% model(TSLM(capp~lag(capp,1)+p_prap+econexp+nytavg+kg+hb+vetoes+override+intrasum+mbills)) %&gt;% report()\n\nSeries: capp \nModel: TSLM \n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4892 -1.3161 -0.1978  1.7633  6.3624 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.13865    3.38461   2.996  0.00382 ** \nlag(capp, 1)  0.77084    0.05585  13.803  &lt; 2e-16 ***\np_prap        0.04708    0.04024   1.170  0.24610    \neconexp       0.08161    0.03195   2.554  0.01290 *  \nnytavg        0.20342    0.06971   2.918  0.00477 ** \nkg           -1.61049    1.11745  -1.441  0.15411    \nhb           -4.87343    1.70280  -2.862  0.00559 ** \nvetoes        0.24012    0.08880   2.704  0.00865 ** \noverride     -0.96229    0.55163  -1.744  0.08560 .  \nintrasum     -0.14809    0.12298  -1.204  0.23267    \nmbills       -0.46652    0.28290  -1.649  0.10375    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.968 on 68 degrees of freedom\nMultiple R-squared: 0.8872, Adjusted R-squared: 0.8706\nF-statistic:  53.5 on 10 and 68 DF, p-value: &lt; 2.22e-16\n\n\nThe Breusch-Godfrey test\n\nsapply(c(1:3), function(x) {lmtest::bgtest(lm(capp~lag(capp,1)+p_prap+econexp+nytavg+kg+hb+vetoes+override+intrasum+mbills, data=dgw.ts), order = x)})\n\n             [,1]                                                          \nstatistic    0.5038368                                                     \nparameter    1                                                             \nmethod       \"Breusch-Godfrey test for serial correlation of order up to 1\"\np.value      0.4778191                                                     \ndata.name    character,2                                                   \ncoefficients numeric,12                                                    \nvcov         numeric,144                                                   \n             [,2]                                                          \nstatistic    1.043041                                                      \nparameter    2                                                             \nmethod       \"Breusch-Godfrey test for serial correlation of order up to 2\"\np.value      0.5936172                                                     \ndata.name    character,2                                                   \ncoefficients numeric,13                                                    \nvcov         numeric,169                                                   \n             [,3]                                                          \nstatistic    1.531487                                                      \nparameter    3                                                             \nmethod       \"Breusch-Godfrey test for serial correlation of order up to 3\"\np.value      0.6750225                                                     \ndata.name    character,2                                                   \ncoefficients numeric,14                                                    \nvcov         numeric,196                                                   \n\n\n* An Alternate Measure\n* ADL\nreg capp l.capp p_prap l.p_prap econexp l.econexp nytavg l.nytavg kg hb vetoes override intrasum mbill \nfitstat\nbgodfrey, lag(1 2 3)\n\ndgw.ts %&gt;% model(TSLM(capp~lag(capp, 1)+p_prap+lag(p_prap, 1)+econexp+lag(econexp,1)+nytavg+lag(nytavg, 1)+kg+hb+vetoes+override+intrasum+mbills)) %&gt;% gg_tsresiduals()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nreg capp l.capp p_prap l.p_prap econexp l.econexp nytavg l.nytavg l2.nytavg kg hb vetoes override intrasum mbill \nfitstat\ntest l.p_prap \n\nsummary(lm(capp~ dplyr::lag(capp, 1)+p_prap+dplyr::lag(p_prap, 1)+econexp+ dplyr::lag(econexp, 1)+ nytavg+ dplyr::lag(nytavg, 1) + dplyr::lag(nytavg, 2)+ kg+hb+vetoes+override+intrasum+mbills, data=dgw.data))\n\n\nCall:\nlm(formula = capp ~ dplyr::lag(capp, 1) + p_prap + dplyr::lag(p_prap, \n    1) + econexp + dplyr::lag(econexp, 1) + nytavg + dplyr::lag(nytavg, \n    1) + dplyr::lag(nytavg, 2) + kg + hb + vetoes + override + \n    intrasum + mbills, data = dgw.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2813 -1.1587 -0.2843  1.5055  5.5623 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            13.255444   4.027775   3.291  0.00164 ** \ndplyr::lag(capp, 1)     0.726775   0.065844  11.038 2.31e-16 ***\np_prap                  0.138223   0.057373   2.409  0.01892 *  \ndplyr::lag(p_prap, 1)  -0.093813   0.055274  -1.697  0.09459 .  \neconexp                 0.005848   0.070969   0.082  0.93459    \ndplyr::lag(econexp, 1)  0.082163   0.072223   1.138  0.25959    \nnytavg                  0.192404   0.071465   2.692  0.00908 ** \ndplyr::lag(nytavg, 1)   0.023006   0.069422   0.331  0.74144    \ndplyr::lag(nytavg, 2)   0.138535   0.078110   1.774  0.08097 .  \nkg                     -1.566093   1.122181  -1.396  0.16774    \nhb                     -4.340598   1.715171  -2.531  0.01389 *  \nvetoes                  0.283759   0.101779   2.788  0.00700 ** \noverride               -1.061008   0.580564  -1.828  0.07235 .  \nintrasum               -0.107917   0.122254  -0.883  0.38074    \nmbills                 -0.715715   0.304646  -2.349  0.02196 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.914 on 63 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8992,    Adjusted R-squared:  0.8768 \nF-statistic: 40.14 on 14 and 63 DF,  p-value: &lt; 2.2e-16\n\n\n\nanova(lm(capp~ dplyr::lag(capp, 1)+p_prap+econexp+ dplyr::lag(econexp, 1)+ nytavg+ dplyr::lag(nytavg, 1) + dplyr::lag(nytavg, 2)+ kg+hb+vetoes+override+intrasum+mbills, data=dgw.data), lm(capp~ dplyr::lag(capp, 1)+p_prap+dplyr::lag(p_prap, 1)+econexp+ dplyr::lag(econexp, 1)+ nytavg+ dplyr::lag(nytavg, 1) + dplyr::lag(nytavg, 2)+ kg+hb+vetoes+override+intrasum+mbills, data=dgw.data)) \n\nAnalysis of Variance Table\n\nModel 1: capp ~ dplyr::lag(capp, 1) + p_prap + econexp + dplyr::lag(econexp, \n    1) + nytavg + dplyr::lag(nytavg, 1) + dplyr::lag(nytavg, \n    2) + kg + hb + vetoes + override + intrasum + mbills\nModel 2: capp ~ dplyr::lag(capp, 1) + p_prap + dplyr::lag(p_prap, 1) + \n    econexp + dplyr::lag(econexp, 1) + nytavg + dplyr::lag(nytavg, \n    1) + dplyr::lag(nytavg, 2) + kg + hb + vetoes + override + \n    intrasum + mbills\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     64 559.29                              \n2     63 534.83  1    24.454 2.8806 0.09459 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n*ECM\nreg d.capp l.capp d.p_prap l.p_prap d.econexp l.econexp d.nytavg l.nytavg kg hb vetoes override intrasum mbill \nbgodfrey, lag(1 2 3)\n\nsummary(lm(difference(capp)~dplyr::lag(capp, 1) + difference(p_prap) +  dplyr::lag(p_prap, 1)+ difference(econexp)+lag(econexp, 1)+difference(nytavg) + dplyr::lag(nytavg) +  kg +  hb +  vetoes + override +  intrasum +  mbills, data=dgw.data))\n\n\nCall:\nlm(formula = difference(capp) ~ dplyr::lag(capp, 1) + difference(p_prap) + \n    dplyr::lag(p_prap, 1) + difference(econexp) + lag(econexp, \n    1) + difference(nytavg) + dplyr::lag(nytavg) + kg + hb + \n    vetoes + override + intrasum + mbills, data = dgw.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0575 -1.2991 -0.2338  1.6723  6.0297 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            9.92398    3.59485   2.761 0.007490 ** \ndplyr::lag(capp, 1)   -0.22348    0.05979  -3.738 0.000394 ***\ndifference(p_prap)     0.11480    0.05548   2.069 0.042508 *  \ndplyr::lag(p_prap, 1)  0.02166    0.04356   0.497 0.620667    \ndifference(econexp)    0.02214    0.07027   0.315 0.753732    \nlag(econexp, 1)        0.08156    0.03311   2.463 0.016420 *  \ndifference(nytavg)     0.18074    0.07179   2.518 0.014290 *  \ndplyr::lag(nytavg)     0.22229    0.09152   2.429 0.017921 *  \nkg                    -1.36022    1.12380  -1.210 0.230519    \nhb                    -4.24191    1.72938  -2.453 0.016865 *  \nvetoes                 0.20605    0.09261   2.225 0.029567 *  \noverride              -0.75686    0.55956  -1.353 0.180870    \nintrasum              -0.12549    0.12277  -1.022 0.310504    \nmbills                -0.53209    0.28906  -1.841 0.070217 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.939 on 65 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4115,    Adjusted R-squared:  0.2938 \nF-statistic: 3.496 on 13 and 65 DF,  p-value: 0.0003862\n\n\n*Bewley\nivreg capp p_prap d.p_prap econexp d.econexp nytavg d.nytavg kg hb vetoes override intrasum mbill (d.capp = l.capp p_prap l.p_prap econexp l.econexp nytavg l.nytavg)",
    "crumbs": [
      "Welcome!",
      "Week 1",
      "Day 4: Cointegration"
    ]
  },
  {
    "objectID": "posts/day-8/index.html",
    "href": "posts/day-8/index.html",
    "title": "Day 8: Panel Models for Limited Outcomes",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation\n\nTwo core issues remain: the spatial and the temporal. There are fewer well worked diagnostics for this than for the pure linear case owing to partial observability and/or coarse measurement in some fashion or other.",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 8: Panel Models for Limited Outcomes"
    ]
  },
  {
    "objectID": "posts/day-8/index.html#slides",
    "href": "posts/day-8/index.html#slides",
    "title": "Day 8: Panel Models for Limited Outcomes",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation\n\nTwo core issues remain: the spatial and the temporal. There are fewer well worked diagnostics for this than for the pure linear case owing to partial observability and/or coarse measurement in some fashion or other.",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 8: Panel Models for Limited Outcomes"
    ]
  },
  {
    "objectID": "posts/day-8/index.html#panel-glm-computing",
    "href": "posts/day-8/index.html#panel-glm-computing",
    "title": "Day 8: Panel Models for Limited Outcomes",
    "section": "Panel GLM Computing",
    "text": "Panel GLM Computing",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 8: Panel Models for Limited Outcomes"
    ]
  },
  {
    "objectID": "posts/day-8/index.html#r",
    "href": "posts/day-8/index.html#r",
    "title": "Day 8: Panel Models for Limited Outcomes",
    "section": "R",
    "text": "R\nAll of the computing will take place in the confines of pglm, a package in the spirit of plm for linear models. The help file provides an example of the familiar models. They are specified to match up with the syntax of R’s glm functions with family and link arguments.\n# install.packages(\"pglm\")\nlibrary(pglm)\n## an ordered probit example\ndata('Fairness', package = 'pglm')\nParking &lt;- subset(Fairness, good == 'parking')\nop &lt;- pglm(as.numeric(answer) ~ education + rule,\nParking[1:105, ],\nfamily = ordinal('probit'), R = 5, print.level = 3,\nmethod = 'bfgs', index = 'id', model = \"random\")\n## a binomial (probit) example\ndata('UnionWage', package = 'pglm')\nanb &lt;- pglm(union ~ wage + exper + rural, UnionWage, family = binomial('probit'),\nmodel = \"pooling\", method = \"bfgs\", print.level = 3, R = 5)\n## a gaussian example on unbalanced panel data\ndata(Hedonic, package = \"plm\")\nra &lt;- pglm(mv ~ crim + zn + indus + nox + age + rm, Hedonic, family = gaussian,\nmodel = \"random\", print.level = 3, method = \"nr\", index = \"townid\")\n## some count data models\ndata(\"PatentsRDUS\", package=\"pglm\")\nla &lt;- pglm(patents ~ lag(log(rd), 0:5) + scisect + log(capital72) + factor(year), PatentsRDUS,\nfamily = negbin, model = \"within\", print.level = 3, method = \"nr\",\nindex = c('cusip', 'year'))\nla &lt;- pglm(patents ~ lag(log(rd), 0:5) + scisect + log(capital72) + factor(year), PatentsRDUS,\nfamily = poisson, model = \"pooling\", index = c(\"cusip\", \"year\"),\nprint.level = 0, method=\"nr\")\n## a tobit example\ndata(\"HealthIns\", package=\"pglm\")\nHealthIns$med2 &lt;- HealthIns$med / 1000\nHealthIns2 &lt;- HealthIns[-2209, ]\nset.seed(2)\nsubs &lt;- sample(1:20186, 200, replace = FALSE)\nHealthIns2 &lt;- HealthIns2[subs, ]\nla &lt;- pglm(med ~ mdu + disease + age, HealthIns2,\nmodel = 'random', family = 'tobit', print.level = 0,\nmethod = 'nr', R = 5)",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 8: Panel Models for Limited Outcomes"
    ]
  },
  {
    "objectID": "posts/day-8/index.html#stata",
    "href": "posts/day-8/index.html#stata",
    "title": "Day 8: Panel Models for Limited Outcomes",
    "section": "Stata",
    "text": "Stata\nStata separates out the commands by family/link combination. So there are xtprobit and xtlogit for binary. xtoprobit for ordered probit models and xtologit [with random effects] for ordered outcomes.",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 8: Panel Models for Limited Outcomes"
    ]
  },
  {
    "objectID": "posts/day-8/index.html#events",
    "href": "posts/day-8/index.html#events",
    "title": "Day 8: Panel Models for Limited Outcomes",
    "section": "Events",
    "text": "Events\nA Stata do file on the box is replicated here. First, a use of regular old glm.\n\nlibrary(haven)\nPatents &lt;- read_dta(url(\"https://github.com/robertwwalker/Essex-Data/raw/main/Patents.dta\"))\nMod.1 &lt;- glm(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5, data=Patents, family=\"poisson\")\nsummary(Mod.1)\n\n\nCall:\nglm(formula = PAT ~ LOGR + LOGR1 + LOGR2 + LOGR3 + LOGR4 + LOGR5 + \n    dyear2 + dyear3 + dyear4 + dyear5, family = \"poisson\", data = Patents)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.82831    0.01253 145.861  &lt; 2e-16 ***\nLOGR         0.15251    0.02939   5.189 2.11e-07 ***\nLOGR1        0.02198    0.04083   0.538 0.590297    \nLOGR2        0.04369    0.03802   1.149 0.250557    \nLOGR3        0.08272    0.03568   2.318 0.020427 *  \nLOGR4        0.10397    0.03176   3.274 0.001061 ** \nLOGR5        0.30109    0.02063  14.594  &lt; 2e-16 ***\ndyear2      -0.04399    0.01320  -3.332 0.000862 ***\ndyear3      -0.06043    0.01343  -4.499 6.81e-06 ***\ndyear4      -0.18925    0.01368 -13.830  &lt; 2e-16 ***\ndyear5      -0.22979    0.01363 -16.854  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 144500  on 1729  degrees of freedom\nResidual deviance:  33745  on 1719  degrees of freedom\nAIC: 39807\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nlibrary(MASS)\nMod.2 &lt;- glm.nb(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5, data=Patents)\nsummary(Mod.2)\n\n\nCall:\nglm.nb(formula = PAT ~ LOGR + LOGR1 + LOGR2 + LOGR3 + LOGR4 + \n    LOGR5 + dyear2 + dyear3 + dyear4 + dyear5, data = Patents, \n    init.theta = 1.276776486, link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.19343    0.05851  20.396  &lt; 2e-16 ***\nLOGR         0.40148    0.11591   3.464 0.000533 ***\nLOGR1       -0.11198    0.15672  -0.714 0.474920    \nLOGR2        0.11039    0.14742   0.749 0.453957    \nLOGR3        0.08770    0.13696   0.640 0.521938    \nLOGR4        0.24389    0.12259   1.989 0.046657 *  \nLOGR5        0.17263    0.08478   2.036 0.041729 *  \ndyear2      -0.05346    0.07726  -0.692 0.488943    \ndyear3      -0.05482    0.07790  -0.704 0.481611    \ndyear4      -0.11764    0.07768  -1.514 0.129941    \ndyear5      -0.22006    0.07764  -2.835 0.004589 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.2768) family taken to be 1)\n\n    Null deviance: 7202.0  on 1729  degrees of freedom\nResidual deviance: 1967.8  on 1719  degrees of freedom\nAIC: 11594\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.2768 \n          Std. Err.:  0.0558 \n\n 2 x log-likelihood:  -11570.1870 \n\n\n\nlibrary(pglm)\n\nLoading required package: maxLik\n\n\nLoading required package: miscTools\n\n\n\nPlease cite the 'maxLik' package as:\nHenningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.\n\nIf you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:\nhttps://r-forge.r-project.org/projects/maxlik/\n\n\nLoading required package: plm\n\nPatents.pdata &lt;- pdata.frame(Patents, index=c(\"id\",\"YEAR\"))\nMod.3A &lt;- pglm(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5+LOGK+SCISECT, Patents.pdata, effect=\"individual\", model=\"random\", family=\"poisson\")\nsummary(Mod.3A)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 4 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-Likelihood: -5234.927 \n14  free parameters\nEstimates:\n            Estimate Std. error t value  Pr(&gt; t)    \n(Intercept)  0.41079    0.14674   2.799 0.005121 ** \nLOGR         0.40345    0.04350   9.274  &lt; 2e-16 ***\nLOGR1       -0.04618    0.04822  -0.958 0.338277    \nLOGR2        0.10792    0.04471   2.414 0.015788 *  \nLOGR3        0.02977    0.04132   0.720 0.471221    \nLOGR4        0.01070    0.03771   0.284 0.776679    \nLOGR5        0.04061    0.03157   1.286 0.198364    \ndyear2      -0.04496    0.01313  -3.425 0.000616 ***\ndyear3      -0.04839    0.01340  -3.610 0.000306 ***\ndyear4      -0.17416    0.01397 -12.467  &lt; 2e-16 ***\ndyear5      -0.22590    0.01466 -15.404  &lt; 2e-16 ***\nLOGK         0.29169    0.03934   7.415 1.21e-13 ***\nSCISECT      0.25700    0.11227   2.289 0.022074 *  \nsigma        1.16969    0.09471  12.350  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\n\nMod.3B &lt;- pglm(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5, Patents.pdata, effect=\"individual\", model=\"random\", family=\"poisson\")\nsummary(Mod.3B)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 5 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -5263.611 \n12  free parameters\nEstimates:\n             Estimate Std. error t value  Pr(&gt; t)    \n(Intercept)  1.402861   0.067052  20.922  &lt; 2e-16 ***\nLOGR         0.476575   0.042263  11.276  &lt; 2e-16 ***\nLOGR1       -0.007708   0.047934  -0.161 0.872242    \nLOGR2        0.136413   0.044734   3.049 0.002293 ** \nLOGR3        0.059191   0.041279   1.434 0.151598    \nLOGR4        0.027518   0.037601   0.732 0.464267    \nLOGR5        0.082547   0.030977   2.665 0.007704 ** \ndyear2      -0.046879   0.013131  -3.570 0.000357 ***\ndyear3      -0.056088   0.013362  -4.198  2.7e-05 ***\ndyear4      -0.190312   0.013789 -13.802  &lt; 2e-16 ***\ndyear5      -0.252677   0.014197 -17.798  &lt; 2e-16 ***\nsigma        1.154854   0.094192  12.261  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\n\nMod.3C &lt;- pglm(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5+LOGK+SCISECT-1, Patents.pdata, effect=\"individual\", model=\"random\", family=\"poisson\")\nsummary(Mod.3C)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 4 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -5238.845 \n13  free parameters\nEstimates:\n         Estimate Std. error t value  Pr(&gt; t)    \nLOGR     0.382042   0.042877   8.910  &lt; 2e-16 ***\nLOGR1   -0.055620   0.048157  -1.155 0.248098    \nLOGR2    0.100308   0.044621   2.248 0.024576 *  \nLOGR3    0.020949   0.041203   0.508 0.611146    \nLOGR4    0.005334   0.037709   0.141 0.887523    \nLOGR5    0.028364   0.031336   0.905 0.365383    \ndyear2  -0.042732   0.013109  -3.260 0.001115 ** \ndyear3  -0.044442   0.013332  -3.333 0.000858 ***\ndyear4  -0.167871   0.013796 -12.168  &lt; 2e-16 ***\ndyear5  -0.216749   0.014303 -15.154  &lt; 2e-16 ***\nLOGK     0.388567   0.019946  19.481  &lt; 2e-16 ***\nSCISECT  0.419111   0.097521   4.298 1.73e-05 ***\nsigma    1.117773   0.088520  12.627  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\n\nMod.4A &lt;- pglm(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5+LOGK+SCISECT, Patents.pdata, effect=\"individual\", model=\"random\", family=\"negbin\")\nsummary(Mod.4A)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 6 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -4948.494 \n15  free parameters\nEstimates:\n             Estimate Std. error t value  Pr(&gt; t)    \n(Intercept)  0.899562   0.168111   5.351 8.75e-08 ***\nLOGR         0.350312   0.065282   5.366 8.04e-08 ***\nLOGR1       -0.003032   0.075092  -0.040 0.967796    \nLOGR2        0.104988   0.068849   1.525 0.127284    \nLOGR3        0.016352   0.063638   0.257 0.797210    \nLOGR4        0.035942   0.058716   0.612 0.540445    \nLOGR5        0.071832   0.048289   1.488 0.136867    \ndyear2      -0.043674   0.021343  -2.046 0.040734 *  \ndyear3      -0.055660   0.021857  -2.547 0.010881 *  \ndyear4      -0.183105   0.022718  -8.060 7.64e-16 ***\ndyear5      -0.230044   0.023152  -9.936  &lt; 2e-16 ***\nLOGK         0.161937   0.041787   3.875 0.000107 ***\nSCISECT      0.117642   0.106616   1.103 0.269848    \na            2.685210   0.258163  10.401  &lt; 2e-16 ***\nb            2.015688   0.217631   9.262  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\n\nMod.4B &lt;- pglm(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5, Patents.pdata, effect=\"individual\", model=\"random\", family=\"negbin\")\nsummary(Mod.4B)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 6 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -4956.259 \n13  free parameters\nEstimates:\n            Estimate Std. error t value  Pr(&gt; t)    \n(Intercept)  1.39944    0.10060  13.910  &lt; 2e-16 ***\nLOGR         0.38762    0.06492   5.970 2.37e-09 ***\nLOGR1        0.01833    0.07597   0.241  0.80935    \nLOGR2        0.12909    0.06972   1.852  0.06408 .  \nLOGR3        0.02469    0.06446   0.383  0.70172    \nLOGR4        0.05775    0.05873   0.983  0.32544    \nLOGR5        0.10003    0.04750   2.106  0.03521 *  \ndyear2      -0.04671    0.02180  -2.142  0.03219 *  \ndyear3      -0.06512    0.02223  -2.930  0.00339 ** \ndyear4      -0.20006    0.02282  -8.765  &lt; 2e-16 ***\ndyear5      -0.25096    0.02298 -10.921  &lt; 2e-16 ***\na            2.64396    0.25190  10.496  &lt; 2e-16 ***\nb            2.02183    0.21889   9.237  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\n\nMod.4C &lt;- pglm(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5+LOGK+SCISECT-1, Patents.pdata, effect=\"individual\", model=\"random\", family=\"negbin\")\nsummary(Mod.4C)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 6 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -4963.305 \n14  free parameters\nEstimates:\n         Estimate Std. error t value  Pr(&gt; t)    \nLOGR     0.296803   0.066943   4.434 9.26e-06 ***\nLOGR1   -0.005170   0.078300  -0.066   0.9474    \nLOGR2    0.085684   0.071482   1.199   0.2307    \nLOGR3    0.004177   0.065959   0.063   0.9495    \nLOGR4    0.010518   0.060898   0.173   0.8629    \nLOGR5    0.048459   0.050342   0.963   0.3357    \ndyear2  -0.032498   0.021569  -1.507   0.1319    \ndyear3  -0.037854   0.021894  -1.729   0.0838 .  \ndyear4  -0.159944   0.022586  -7.082 1.42e-12 ***\ndyear5  -0.201171   0.022791  -8.827  &lt; 2e-16 ***\nLOGK     0.343399   0.027143  12.652  &lt; 2e-16 ***\nSCISECT  0.427951   0.089606   4.776 1.79e-06 ***\na        2.435387   0.225784  10.786  &lt; 2e-16 ***\nb        2.154018   0.240574   8.954  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 8: Panel Models for Limited Outcomes"
    ]
  },
  {
    "objectID": "posts/day-8/index.html#the-gee",
    "href": "posts/day-8/index.html#the-gee",
    "title": "Day 8: Panel Models for Limited Outcomes",
    "section": "The GEE",
    "text": "The GEE\n\nlibrary(geepack)\ngee.Mod &lt;- geeglm(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5, data=Patents, id=id, corstr=\"exchangeable\", family=poisson)\nsummary(gee.Mod)\n\n\nCall:\ngeeglm(formula = PAT ~ LOGR + LOGR1 + LOGR2 + LOGR3 + LOGR4 + \n    LOGR5 + dyear2 + dyear3 + dyear4 + dyear5, family = poisson, \n    data = Patents, id = id, corstr = \"exchangeable\")\n\n Coefficients:\n             Estimate   Std.err    Wald Pr(&gt;|W|)    \n(Intercept)  1.851156  0.122963 226.638  &lt; 2e-16 ***\nLOGR         0.392713  0.064716  36.824 1.29e-09 ***\nLOGR1        0.001683  0.057782   0.001  0.97677    \nLOGR2        0.132732  0.048185   7.588  0.00588 ** \nLOGR3        0.039535  0.064560   0.375  0.54029    \nLOGR4        0.036056  0.049440   0.532  0.46582    \nLOGR5        0.081190  0.047667   2.901  0.08851 .  \ndyear2      -0.048400  0.017823   7.375  0.00661 ** \ndyear3      -0.056190  0.027314   4.232  0.03967 *  \ndyear4      -0.191152  0.042825  19.923 8.06e-06 ***\ndyear5      -0.248547  0.041809  35.341 2.77e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation structure = exchangeable \nEstimated Scale Parameters:\n\n            Estimate Std.err\n(Intercept)    22.57   4.102\n  Link = identity \n\nEstimated Correlation Parameters:\n      Estimate Std.err\nalpha   0.8983 0.06245\nNumber of clusters:   346  Maximum cluster size: 5 \n\n\n\n\ngee.Mod &lt;- geeglm(PAT~LOGR+LOGR1+LOGR2+LOGR3+LOGR4+LOGR5+dyear2+dyear3+dyear4+dyear5, data=Patents, id=id, corstr=\"ar1\", family=poisson)\nsummary(gee.Mod)\n\n\nCall:\ngeeglm(formula = PAT ~ LOGR + LOGR1 + LOGR2 + LOGR3 + LOGR4 + \n    LOGR5 + dyear2 + dyear3 + dyear4 + dyear5, family = poisson, \n    data = Patents, id = id, corstr = \"ar1\")\n\n Coefficients:\n            Estimate Std.err   Wald Pr(&gt;|W|)    \n(Intercept)   1.8121  0.1270 203.72  &lt; 2e-16 ***\nLOGR          0.3096  0.0554  31.17  2.4e-08 ***\nLOGR1         0.0254  0.0549   0.21   0.6435    \nLOGR2         0.1624  0.0518   9.82   0.0017 ** \nLOGR3         0.0801  0.0564   2.02   0.1555    \nLOGR4         0.0793  0.0446   3.16   0.0756 .  \nLOGR5         0.0408  0.0411   0.99   0.3200    \ndyear2       -0.0494  0.0175   8.01   0.0047 ** \ndyear3       -0.0542  0.0266   4.14   0.0419 *  \ndyear4       -0.1817  0.0427  18.08  2.1e-05 ***\ndyear5       -0.2333  0.0409  32.48  1.2e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation structure = ar1 \nEstimated Scale Parameters:\n\n            Estimate Std.err\n(Intercept)     22.3    4.02\n  Link = identity \n\nEstimated Correlation Parameters:\n      Estimate Std.err\nalpha    0.948  0.0265\nNumber of clusters:   346  Maximum cluster size: 5 \n\n\nAnd an anova…\n\nanova(gee.Mod)\n\nAnalysis of 'Wald statistic' Table\nModel: poisson, link: log\nResponse: PAT\nTerms added sequentially (first to last)\n\n       Df  X2 P(&gt;|Chi|)    \nLOGR    1 368   &lt; 2e-16 ***\nLOGR1   1  23   1.9e-06 ***\nLOGR2   1  19   1.4e-05 ***\nLOGR3   1  16   5.8e-05 ***\nLOGR4   1   7   0.00708 ** \nLOGR5   1   1   0.28590    \ndyear2  1   4   0.05740 .  \ndyear3  1  11   0.00099 ***\ndyear4  1   0   0.67359    \ndyear5  1  32   1.2e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 8: Panel Models for Limited Outcomes"
    ]
  },
  {
    "objectID": "posts/day-8/index.html#bkt-and-carter-signorino",
    "href": "posts/day-8/index.html#bkt-and-carter-signorino",
    "title": "Day 8: Panel Models for Limited Outcomes",
    "section": "BKT and Carter-Signorino",
    "text": "BKT and Carter-Signorino\nLoad the data\n\nBKT.data &lt;- read_dta(url(\"https://github.com/robertwwalker/Essex-Data/raw/main/bkt98ajps.dta\"))\nMod.1 &lt;- glm(dispute~ dem+growth+allies+contig+capratio+trade, family=binomial(\"logit\"), data=BKT.data)\nsummary(Mod.1)\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade, family = binomial(\"logit\"), data = BKT.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.29e+00   7.92e-02  -41.54  &lt; 2e-16 ***\ndem         -4.97e-02   7.38e-03   -6.73  1.7e-11 ***\ngrowth      -2.23e-02   8.52e-03   -2.62   0.0088 ** \nallies      -8.21e-01   8.00e-02  -10.26  &lt; 2e-16 ***\ncontig       1.31e+00   7.96e-02   16.49  &lt; 2e-16 ***\ncapratio    -3.07e-03   4.17e-04   -7.37  1.8e-13 ***\ntrade       -6.61e+01   1.34e+01   -4.92  8.6e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7719.2  on 20989  degrees of freedom\nResidual deviance: 6955.1  on 20983  degrees of freedom\nAIC: 6969\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nMod.1B &lt;- glm(dispute~ dem+growth+allies+contig+capratio+trade+as.factor(py), family=binomial(\"logit\"), data=BKT.data)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(Mod.1B)\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade + as.factor(py), family = binomial(\"logit\"), data = BKT.data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -9.43e-01   9.28e-02  -10.16  &lt; 2e-16 ***\ndem             -5.47e-02   8.00e-03   -6.84  8.0e-12 ***\ngrowth          -1.15e-02   9.24e-03   -1.24  0.21492    \nallies          -4.71e-01   9.00e-02   -5.23  1.7e-07 ***\ncontig           6.99e-01   8.93e-02    7.82  5.3e-15 ***\ncapratio        -3.03e-03   4.17e-04   -7.28  3.3e-13 ***\ntrade           -1.27e+01   1.05e+01   -1.21  0.22735    \nas.factor(py)2  -2.04e+00   1.41e-01  -14.48  &lt; 2e-16 ***\nas.factor(py)3  -2.15e+00   1.54e-01  -13.92  &lt; 2e-16 ***\nas.factor(py)4  -2.37e+00   1.74e-01  -13.64  &lt; 2e-16 ***\nas.factor(py)5  -2.62e+00   2.01e-01  -13.04  &lt; 2e-16 ***\nas.factor(py)6  -3.09e+00   2.52e-01  -12.25  &lt; 2e-16 ***\nas.factor(py)7  -3.08e+00   2.52e-01  -12.21  &lt; 2e-16 ***\nas.factor(py)8  -3.00e+00   2.46e-01  -12.23  &lt; 2e-16 ***\nas.factor(py)9  -2.95e+00   2.46e-01  -11.98  &lt; 2e-16 ***\nas.factor(py)10 -3.24e+00   2.86e-01  -11.32  &lt; 2e-16 ***\nas.factor(py)11 -3.45e+00   3.24e-01  -10.66  &lt; 2e-16 ***\nas.factor(py)12 -3.32e+00   3.10e-01  -10.72  &lt; 2e-16 ***\nas.factor(py)13 -3.92e+00   4.14e-01   -9.46  &lt; 2e-16 ***\nas.factor(py)14 -4.56e+00   5.82e-01   -7.84  4.6e-15 ***\nas.factor(py)15 -4.58e+00   5.82e-01   -7.87  3.5e-15 ***\nas.factor(py)16 -3.45e+00   3.41e-01  -10.12  &lt; 2e-16 ***\nas.factor(py)17 -4.22e+00   5.05e-01   -8.35  &lt; 2e-16 ***\nas.factor(py)18 -5.59e+00   1.00e+00   -5.57  2.5e-08 ***\nas.factor(py)19 -3.61e+00   3.85e-01   -9.36  &lt; 2e-16 ***\nas.factor(py)20 -3.95e+00   4.53e-01   -8.71  &lt; 2e-16 ***\nas.factor(py)21 -4.80e+00   7.11e-01   -6.74  1.5e-11 ***\nas.factor(py)22 -4.31e+00   5.82e-01   -7.39  1.4e-13 ***\nas.factor(py)23 -4.13e+00   5.83e-01   -7.08  1.4e-12 ***\nas.factor(py)24 -3.99e+00   5.83e-01   -6.84  7.7e-12 ***\nas.factor(py)25 -1.76e+01   3.31e+02   -0.05  0.95752    \nas.factor(py)26 -2.97e+00   4.19e-01   -7.10  1.2e-12 ***\nas.factor(py)27 -1.76e+01   3.85e+02   -0.05  0.96353    \nas.factor(py)28 -1.76e+01   3.83e+02   -0.05  0.96347    \nas.factor(py)29 -3.92e+00   7.14e-01   -5.49  3.9e-08 ***\nas.factor(py)30 -3.42e+00   5.86e-01   -5.84  5.2e-09 ***\nas.factor(py)31 -4.41e+00   1.01e+00   -4.39  1.1e-05 ***\nas.factor(py)32 -3.57e+00   7.15e-01   -4.99  6.0e-07 ***\nas.factor(py)33 -4.07e+00   1.01e+00   -4.05  5.2e-05 ***\nas.factor(py)34 -3.31e+00   7.16e-01   -4.61  3.9e-06 ***\nas.factor(py)35 -3.83e+00   1.01e+00   -3.81  0.00014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7719.2  on 20989  degrees of freedom\nResidual deviance: 5109.4  on 20949  degrees of freedom\nAIC: 5191\n\nNumber of Fisher Scoring iterations: 17\n\n\n\nMod.1C &lt;- glm(dispute~ dem+growth+allies+contig+capratio+trade+py+pys1+pys2+pys3, family=binomial(\"logit\"), data=BKT.data)\nsummary(Mod.1C)\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade + py + pys1 + pys2 + pys3, family = binomial(\"logit\"), \n    data = BKT.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  8.50e-01   1.60e-01    5.31  1.1e-07 ***\ndem         -5.46e-02   7.97e-03   -6.84  7.7e-12 ***\ngrowth      -1.15e-02   9.20e-03   -1.25     0.21    \nallies      -4.70e-01   8.96e-02   -5.25  1.5e-07 ***\ncontig       6.94e-01   8.90e-02    7.79  6.6e-15 ***\ncapratio    -3.04e-03   4.17e-04   -7.29  3.0e-13 ***\ntrade       -1.29e+01   1.05e+01   -1.23     0.22    \npy          -1.82e+00   1.11e-01  -16.30  &lt; 2e-16 ***\npys1        -2.45e-01   2.61e-02   -9.37  &lt; 2e-16 ***\npys2         7.92e-02   1.09e-02    7.24  4.5e-13 ***\npys3        -1.09e-02   2.76e-03   -3.96  7.5e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7719.2  on 20989  degrees of freedom\nResidual deviance: 5165.8  on 20979  degrees of freedom\nAIC: 5188\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nMod.2A &lt;- glm(dispute~ dem+growth+allies+contig+capratio+trade, family=binomial(\"logit\"), subset = contdisp!=1, data=BKT.data)\nsummary(Mod.2A)\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade, family = binomial(\"logit\"), data = BKT.data, subset = contdisp != \n    1)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.33e+00   1.15e-01  -37.78  &lt; 2e-16 ***\ndem         -4.01e-02   1.01e-02   -3.99  6.7e-05 ***\ngrowth      -3.43e-02   1.25e-02   -2.74   0.0062 ** \nallies      -4.80e-01   1.13e-01   -4.25  2.1e-05 ***\ncontig       1.35e+00   1.21e-01   11.20  &lt; 2e-16 ***\ncapratio    -1.96e-03   5.01e-04   -3.92  9.0e-05 ***\ntrade       -2.11e+01   1.13e+01   -1.86   0.0623 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3978.5  on 20447  degrees of freedom\nResidual deviance: 3693.8  on 20441  degrees of freedom\nAIC: 3708\n\nNumber of Fisher Scoring iterations: 9\n\n\n\nMod.2B &lt;- glm(dispute~ dem+growth+allies+contig+capratio+trade+py+pys1+pys2+pys3, family=binomial(\"logit\"), subset = contdisp!=1, data=BKT.data)\nsummary(Mod.2B)\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade + py + pys1 + pys2 + pys3, family = binomial(\"logit\"), \n    data = BKT.data, subset = contdisp != 1)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.956674   0.297828  -13.29  &lt; 2e-16 ***\ndem         -0.038809   0.010046   -3.86  0.00011 ***\ngrowth      -0.040144   0.012502   -3.21  0.00132 ** \nallies      -0.365957   0.114272   -3.20  0.00136 ** \ncontig       0.993251   0.124133    8.00  1.2e-15 ***\ncapratio    -0.002289   0.000523   -4.38  1.2e-05 ***\ntrade       -3.809220   9.679795   -0.39  0.69393    \npy           0.390965   0.157920    2.48  0.01330 *  \npys1         0.089719   0.031787    2.82  0.00477 ** \npys2        -0.028888   0.012634   -2.29  0.02222 *  \npys3         0.003314   0.002974    1.11  0.26513    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3978.5  on 20447  degrees of freedom\nResidual deviance: 3502.8  on 20437  degrees of freedom\nAIC: 3525\n\nNumber of Fisher Scoring iterations: 9\n\n\n\nMod.3A &lt;- glm(dispute~ dem+growth+allies+contig+capratio+trade+py+pys1+pys2+pys3, family=binomial(\"logit\"), subset = prefail&lt;1, data=BKT.data)\nsummary(Mod.3A)\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade + py + pys1 + pys2 + pys3, family = binomial(\"logit\"), \n    data = BKT.data, subset = prefail &lt; 1)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.132791   0.369090   -5.78  7.5e-09 ***\ndem         -0.046143   0.013330   -3.46  0.00054 ***\ngrowth      -0.022954   0.017754   -1.29  0.19604    \nallies      -0.423113   0.163691   -2.58  0.00974 ** \ncontig       1.105361   0.172537    6.41  1.5e-10 ***\ncapratio    -0.001916   0.000601   -3.19  0.00142 ** \ntrade       -3.553603  11.726412   -0.30  0.76186    \npy          -1.081689   0.235688   -4.59  4.4e-06 ***\npys1        -0.183730   0.050096   -3.67  0.00024 ***\npys2         0.069295   0.019788    3.50  0.00046 ***\npys3        -0.013987   0.004484   -3.12  0.00181 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2183.3  on 16990  degrees of freedom\nResidual deviance: 1928.1  on 16980  degrees of freedom\nAIC: 1950\n\nNumber of Fisher Scoring iterations: 9\n\n\n\nCarter and Signorino\nUse a Taylor series to clean this up with a suggestion that no baseline hazard is more than cubic [two inflection points].\n\nMod.CS &lt;- glm(dispute~ dem+growth+allies+contig+capratio+trade+prefail+py+I(py^2)+I(py^3), family=binomial(\"logit\"), data=BKT.data)\nsummary(Mod.CS)\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade + prefail + py + I(py^2) + I(py^3), family = binomial(\"logit\"), \n    data = BKT.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.14e+00   1.18e-01   -9.64  &lt; 2e-16 ***\ndem         -4.09e-02   8.23e-03   -4.97  6.7e-07 ***\ngrowth      -2.44e-02   9.55e-03   -2.56   0.0105 *  \nallies      -2.67e-01   9.20e-02   -2.90   0.0038 ** \ncontig       6.62e-01   9.20e-02    7.19  6.4e-13 ***\ncapratio    -2.00e-03   3.71e-04   -5.40  6.6e-08 ***\ntrade       -1.06e+01   1.04e+01   -1.03   0.3049    \nprefail      1.75e-01   8.96e-03   19.53  &lt; 2e-16 ***\npy          -7.48e-01   4.14e-02  -18.08  &lt; 2e-16 ***\nI(py^2)      4.22e-02   3.72e-03   11.34  &lt; 2e-16 ***\nI(py^3)     -7.18e-04   8.76e-05   -8.19  2.5e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7719.2  on 20989  degrees of freedom\nResidual deviance: 4909.5  on 20979  degrees of freedom\nAIC: 4932\n\nNumber of Fisher Scoring iterations: 8\n\n\nAs a technical matter, none of this is exactly equivalent to a Cox model. That requires the cloglog.\n\nMod.CS &lt;- glm(dispute~ dem+growth+allies+contig+capratio+trade+prefail+py+I(py^2)+I(py^3), family=binomial(\"cloglog\"), data=BKT.data)\nsummary(Mod.CS)\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade + prefail + py + I(py^2) + I(py^3), family = binomial(\"cloglog\"), \n    data = BKT.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.13e+00   1.07e-01  -10.62  &lt; 2e-16 ***\ndem         -4.16e-02   7.50e-03   -5.54  2.9e-08 ***\ngrowth      -1.97e-02   8.00e-03   -2.47   0.0137 *  \nallies      -2.33e-01   7.99e-02   -2.92   0.0035 ** \ncontig       5.16e-01   7.86e-02    6.56  5.4e-11 ***\ncapratio    -2.13e-03   3.60e-04   -5.93  3.0e-09 ***\ntrade       -1.15e+01   1.01e+01   -1.13   0.2567    \nprefail      1.15e-01   6.46e-03   17.80  &lt; 2e-16 ***\npy          -7.03e-01   3.84e-02  -18.32  &lt; 2e-16 ***\nI(py^2)      3.89e-02   3.51e-03   11.07  &lt; 2e-16 ***\nI(py^3)     -6.55e-04   8.33e-05   -7.85  4.1e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7719.2  on 20989  degrees of freedom\nResidual deviance: 4944.0  on 20979  degrees of freedom\nAIC: 4966\n\nNumber of Fisher Scoring iterations: 9",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 8: Panel Models for Limited Outcomes"
    ]
  },
  {
    "objectID": "posts/day-6/index.html",
    "href": "posts/day-6/index.html",
    "title": "Day 6: Panel Data",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 6: Panel Data"
    ]
  },
  {
    "objectID": "posts/day-6/index.html#slides",
    "href": "posts/day-6/index.html#slides",
    "title": "Day 6: Panel Data",
    "section": "",
    "text": "Slides in .pdf format\nA xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 6: Panel Data"
    ]
  },
  {
    "objectID": "posts/day-6/index.html#panelr",
    "href": "posts/day-6/index.html#panelr",
    "title": "Day 6: Panel Data",
    "section": "panelr",
    "text": "panelr\nThe panelr package vignette on between-within\nStarting the panel data, or the generalization to multiple time series, perhaps the most famous question in the generic literature is a question about fixed and random effects, more precisely, do we estimate specific unobserved constants or do we seek only the distribution of these constants. The implications of this basic issue are substantial.",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 6: Panel Data"
    ]
  },
  {
    "objectID": "posts/day-6/index.html#some-simulated-data",
    "href": "posts/day-6/index.html#some-simulated-data",
    "title": "Day 6: Panel Data",
    "section": "Some Simulated Data",
    "text": "Some Simulated Data\nRandom effects and pooled regressions can be terribly wrong when the pooled and random effects moment condition fails. Let’s show some data here to illustrate the point. The true model here is \\[ y_{it} = \\alpha_{i} + X_{it}\\beta + \\epsilon_{it} \\] where the \\(\\beta=1\\) and \\(\\alpha_{i}=\\{6,0,-6\\}\\) and \\(\\epsilon \\sim \\mathcal{N}(0,1)\\). Here is the plot.\n\nX.FE &lt;- c(seq(-2.5,-0.5,by=0.05),seq(-2,0,by=0.05),seq(-1.5,0.5,by=0.05))\ny.FE &lt;- -3*c(rep(-2,41),rep(0,41),rep(2,41))+X.FE + rnorm(123,0,1)\nFE.data &lt;- data.frame(y.FE,X.FE,unit=c(rep(1,41),rep(2,41),rep(3,41)), time=rep(seq(1,41,1),3))\nlibrary(foreign)\nwrite.dta(FE.data, \"FEData-2.dta\")\npar(mfrow=c(1,2))\nwith(FE.data, plot(X.FE,y.FE, bty=\"n\", main=\"Pooled\"))\nwith(FE.data, abline(lm(y.FE~X.FE), lty=2, col=\"brown\"))\nwith(FE.data, plot(X.FE,y.FE, bty=\"n\", col=unit, main=\"Fixed Effects\"))\nabline(a=-6,b=1, col=\"blue\")\nabline(a=0,b=1, col=\"blue\")\nabline(a=6,b=1, col=\"blue\")",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 6: Panel Data"
    ]
  },
  {
    "objectID": "posts/day-6/index.html#three-models",
    "href": "posts/day-6/index.html#three-models",
    "title": "Day 6: Panel Data",
    "section": "Three Models",
    "text": "Three Models\n\nlibrary(plm)\nFE.pdata &lt;- pdata.frame(FE.data, c(\"unit\",\"time\"))\nmod.RE &lt;- plm(y.FE~X.FE, data=FE.pdata, model=\"random\")\nmod.RE2 &lt;- plm(y.FE~X.FE, data=FE.pdata, model=\"random\", random.method = \"amemiya\")\nmod.RE3 &lt;- plm(y.FE~X.FE, data=FE.pdata, model=\"random\", random.method = \"walhus\")\nmod.RE4 &lt;- plm(y.FE~X.FE, data=FE.pdata, model=\"random\", random.method = \"nerlove\")\nmod.FE &lt;- plm(y.FE~X.FE, data=FE.pdata, model=\"within\")\nmod.pool &lt;- plm(y.FE~X.FE, data=FE.pdata, model=\"pooling\")",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 6: Panel Data"
    ]
  },
  {
    "objectID": "posts/day-6/index.html#omitted-fixed-effects-can-be-very-bad",
    "href": "posts/day-6/index.html#omitted-fixed-effects-can-be-very-bad",
    "title": "Day 6: Panel Data",
    "section": "Omitted Fixed Effects can be Very Bad",
    "text": "Omitted Fixed Effects can be Very Bad\nAs we can see, the default random effects model in R [and Stata] is actually pretty horrible.\nlibrary(stargazer)\nstargazer(mod.RE,mod.RE2,mod.RE3,mod.RE4,mod.pool,mod.FE, type=\"html\", column.labels=c(\"RE\",\"RE-WalHus\",\"RE-Amemiya\",\"RE-Nerlove\",\"Pooled\",\"FE\"))\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ny.FE\n\n\n\n\n\n\nRE\n\n\nRE-WalHus\n\n\nRE-Amemiya\n\n\nRE-Nerlove\n\n\nPooled\n\n\nFE\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n\n\n\n\n\n\nX.FE\n\n\n-0.633*\n\n\n0.895***\n\n\n0.824***\n\n\n0.897***\n\n\n-2.905***\n\n\n0.900***\n\n\n\n\n\n\n(0.381)\n\n\n(0.134)\n\n\n(0.158)\n\n\n(0.134)\n\n\n(0.513)\n\n\n(0.134)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-1.677***\n\n\n-0.148\n\n\n-0.220\n\n\n-0.147\n\n\n-3.948***\n\n\n\n\n\n\n\n\n(0.576)\n\n\n(2.807)\n\n\n(0.818)\n\n\n(3.465)\n\n\n(0.632)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n123\n\n\n123\n\n\n123\n\n\n123\n\n\n123\n\n\n123\n\n\n\n\nR2\n\n\n0.022\n\n\n0.268\n\n\n0.184\n\n\n0.270\n\n\n0.209\n\n\n0.275\n\n\n\n\nAdjusted R2\n\n\n0.014\n\n\n0.262\n\n\n0.177\n\n\n0.264\n\n\n0.203\n\n\n0.257\n\n\n\n\nF Statistic\n\n\n2.753*\n\n\n44.326***\n\n\n27.240***\n\n\n44.859***\n\n\n32.022*** (df = 1; 121)\n\n\n45.118*** (df = 1; 119)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nDiscussion\nThe random method matters quite a bit though; many of them are very close to the truth. Models containing much or all of the between information are wrong.\nIf the X and unit effects are dependent, then there are serious threats to proper inference.",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 6: Panel Data"
    ]
  },
  {
    "objectID": "posts/day-6/index.html#plm-things",
    "href": "posts/day-6/index.html#plm-things",
    "title": "Day 6: Panel Data",
    "section": "plm things",
    "text": "plm things\nBeck and Katz (1995) standard errors are provided with vcovBK(). The key argument is cluster which averages over groups or time. The Beck and Katz paper would involve cluster=\"time\".\nAlmost all panel unit root testing goes on with purtest. The test= argument is key for IPS, Levin, et al., Maddala-Wu, Hadri, and various tests proposed by Choi (2001). A few others are specified individually below.\n\nThe test of serial correlation for panel models is given by pbgtest(model).\nThe Baltagi and Li test of serial correlation in panel models with random effects is given by pbltest(model). The various alternatives are specified in alternative.\nThe Baltagi-Wu statistic for AR(1) disturbances is given by pbnftest(model, test=\"lbi\") while a BNF (1982) statistic is the default for this test for fixed effects models.\n\n# replicate Baltagi (2013), p. 101, table 5.1:\nre &lt;- plm(inv ~ value + capital, data = Grunfeld, model = \"random\")\npbnftest(re, test = \"lbi\")\n\npbsytest(model) gives the joint test of Baltagi and Li and a variant owing to Bera, et. al (2001) and Sosa-Escudero and Bera (2008) – the latter is a paper in Stata journal with companion software to be installed.\npcdtest(formula, data) gives the Pesaran test for cross-sectional dependence.\npdwtest(model) gives a panel Durbin-Watson statistic.\npFtest gives the F-test of fixed effects.\npggls gives GLS estimators for panel data specifying the effect and a model of within, pooling, fd.\nphansitest(purtest object) combines unit root tests in the method proposed by Hanck (2013).\nphtest(model1, model2) is the Hausman test for panel data models. This one has robust options detailed in the last section of ?phtest.\npiest(formula, data) performs Chamberlain’s tests on the within regression.\nAnother test of unit/time effects is given in plmtest().\nChow tests of poolability are given by pooltest() applied to a pooled or within regression.\npvar ensures variation along dimensions.\npvcm will estimate variable coefficients models ala Swamy (1970).\nJoint tests of coefficients are constructed using pwaldtest.\nWooldridge’s test for serial correlation in within models is pwartest(model)\nWooldridge’s test for AR(1) errors in level or differenced panel models is given by pwfdtest(model). The underlying idea is clever; if the levels are independent then the errors in first-differences will be correlated as -0.5. The test can be implemented against either within/fe or first-difference alternatives.\npwtest(pooling model) gives a semi-parametric test for the presence of (individual or time) unobserved effects in panel models that owes to Wooldridge.\nranef and fixef extract the random and fixed effects, respectively.",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 6: Panel Data"
    ]
  },
  {
    "objectID": "posts/day-10/index.html",
    "href": "posts/day-10/index.html",
    "title": "Day 10: Causal Inference in Panels and TWFE",
    "section": "",
    "text": "A xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 10: Causal Inference in Panels and TWFE"
    ]
  },
  {
    "objectID": "posts/day-10/index.html#slides",
    "href": "posts/day-10/index.html#slides",
    "title": "Day 10: Causal Inference in Panels and TWFE",
    "section": "",
    "text": "A xaringan for presentation",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 10: Causal Inference in Panels and TWFE"
    ]
  },
  {
    "objectID": "posts/day-10/index.html#wrapping-up",
    "href": "posts/day-10/index.html#wrapping-up",
    "title": "Day 10: Causal Inference in Panels and TWFE",
    "section": "Wrapping Up",
    "text": "Wrapping Up",
    "crumbs": [
      "Welcome!",
      "Week 2",
      "Day 10: Causal Inference in Panels and TWFE"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESSSSDA25-2W",
    "section": "",
    "text": "Day 10: Causal Inference in Panels and TWFE\n\n\n\n\n\n\npanel data\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 1, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nDay 9: DPD and GMM\n\n\n\n\n\n\npanel data\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 31, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nDay 8: Panel Models for Limited Outcomes\n\n\n\n\n\n\npanel data\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 30, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nDay 7: Missing Data\n\n\n\n\n\n\npanel data\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 29, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nDay 6: Panel Data\n\n\n\n\n\n\npanel data\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 28, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5: Equation Balance to Wrap Up Week 1\n\n\n\n\n\n\ntime series\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 25, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4: Cointegration\n\n\n\n\n\n\ntime series\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 24, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3: Dynamic Linear, ADL Models, and VARs\n\n\n\n\n\n\ntime series\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 23, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2: Time Series, Stationarity, and ARIMA Models\n\n\n\n\n\n\ntime series\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 22, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1: Time, Decomposition, and Summaries\n\n\n\n\n\n\npanel data\n\ntime series\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 21, 2025\n\n\nRobert W. Walker\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome!\n\n\n2W: Advanced Time Series/Panel\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 14, 2025\n\n\nRobert W. Walker\n\n\n\n\n\nNo matching items"
  }
]